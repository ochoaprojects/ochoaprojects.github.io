[ { "title": "Deploying Kubernetes With K3S and Proxmox", "url": "/posts/DeployingKubernetesWithK3S/", "categories": "Containers, Kubernetes", "tags": "kubernetes, k3s, portainer", "date": "2023-02-28 09:00:00 -0800", "snippet": "Kubernetes (k8s) is the most popular and powerful container-orchestration platform. It lets you rapidly deploy, manage, and maintain applications. K3S is the lightweight and certified Kubernetes distribution that’s perfect for development, CI/CD, edge computing and arm-based architectures. With a reduced footprint and easy setup, K3S can be up and running in minutes - taking Kubernetes anywhere.Let’s dive into what makes K3S so great! Unlike other Kubernetes distributions, K3S packs all of the features you’d expect from a production-ready Kubernetes deployment into a single binary of less than 50MB. This compactness has dramatic benefits when time and space are both limited. Additionally, to get the most out of K3S, it includes a bundle of additional components such as Helm and Traefik.K3S also contains an embedded database, making cluster services quick to start up. This ensures that all your nodes remain in sync with each other. Furthermore, K3S is one of the few distributions to offer support for arm-based architectures. As a cloud-native and low-resource solution, K3S is designed to work well with a wide range of IoT devices and edge computing tasks.K3S’ one-click install and kubeconfig generation means that you can quickly spin up an application ready for use. This benefit decreases setup time and comes in particularly handy when deploying micro-services to multiple cloud and on-premise servers. All of this considered, K3S is one of the easiest ways to get Kubernetes up and running in no time flat.Table of Contents Create a Virtual Machine in Proxmox Install K3S Deplopying Portainer with Kubernetes Adding Nodes to the ClusterCreate a Virtual Machine in ProxmoxOnce again I will be leveraging my terraform code that I have mentioned and provided in previous posts. In the case of k3s I will still deploy 3 nodes so that I can demonstrate adding addtional nodes to the cluster.If you want to check out the terraform code I am using for deployment of my VMs in Proxmox then please check out Deploying Kubernetes With Rancher and Proxmox and Deploying VMs With Terraform In ProxMox.Install K3SLet’s start by visiting the K3S website: k3s.ioCopy the command from their frontpagecurl -sfL https://get.k3s.io | sh -Paste and run the command into your master node and give it a few moments to install.Now to check your K3S node is up and running, run the second command from the site.k3s kubectl get nodeCongrats! This is basically all it takes to get Kubernetes up and running with K3S! However, I want to take it a couple of steps further in this post with K3S, so let’s continue.Deplopying Portainer with KubernetesI am going to be using YAML manifests and the documentation for portainer and Kubernetes can be found here: Install Portainer-CE on KubernetesK3S ships with traefik, which is a load balancer, by default. So we are able to leverage this by choosing the “Expose via Load Balancer” tab and then copying/running the command below. There is a security feature with Portainer that may prevent you from creating the inital admin user password because of timeout reasons. If you run into this issue, it will require that you restart the service. To restart the services simply run the following command: sudo kubectl rollout restart deployment portainer -n portainer Now if we run the following command we will be able to see Portainer running in Kubernetes, what external IP, and what port it is listening on:sudo kubectl get service -ANow we can access Portainer with the following:https://&lt;externalIP&gt;:9443This will bring us to the admin password creation page.Next choose “Get Started”Adding Nodes to the ClusterNow that K3S is installed and running, we have successfully set up Portainer and are ready to complete our cluster. To do so, let’s add some additional nodes to our cluster.The first thing we need to grab for this step is our node token. This can be found at the following directory:NODE_TOKEN comes from /var/lib/rancher/k3s/server/node-token on your serverOnce you have your node token, we can run the following command on the other machines we want to add as nodes for our cluster:curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh -Now we can check Portainer and see our nodes have been added." }, { "title": "Deploying Kubernetes With Rancher and Proxmox", "url": "/posts/DeployingKubernetesWithRancher/", "categories": "Containers, Kubernetes", "tags": "kubernetes, rancher", "date": "2023-02-23 09:00:00 -0800", "snippet": "Rancher is a popular open-source platform that provides a complete solution for managing Kubernetes clusters. With Rancher, users can easily deploy, manage, and scale their Kubernetes clusters across multiple environments, including on-premise, cloud, and hybrid deployments. Rancher also offers a user-friendly interface for managing Kubernetes, making it an ideal solution for teams with varying levels of expertise. In this post, we will explore the features and benefits of Rancher for Kubernetes, as well as how it can help you streamline your Kubernetes management processes.Table of Contents Find a host machine Create 3 or more virtual machines in Proxmox main.tf vars.tf Set-up Update Upgrade Install Docker Set up Rancher K3s Create your Kubernetes Cluster Join your master and worker nodes to the cluster Complete The contents of this post are for demo purposes only. Secrets will be gone before posting and therefore left with intention.Find a host machinePersonally I am deploying this on my Proxmox cluser through a Debian VM. You can choose anything that works for you though.Create 3 or more virtual machines in ProxmoxIt is necessary to generate a minimum of 3 virtual machines for this task. However, I have decided to use 4 virtual machines to have an additional worker node that can efficiently manage my workloads. Among the virtual machines, the GUI node will be assigned for hosting our Rancher GUI, the master node will function as our etcd, scheduling, and control plane, while the worker nodes will take care of running our workloads.In a previous post I went over how to deploy VMs by using Terraform. I will be using that method to deploy my VMs to my Proxmox cluster.Here is what my Terraform code looks like for this method:main.tf#Configure ProxMox provider using Telmate found here https://github.com/Telmate/terraform-provider-proxmoxterraform { required_providers { proxmox = { source = \"telmate/proxmox\" version = \"2.7.4\" } }}#Configure ProxMox API user/permissions and API urlprovider \"proxmox\" { pm_api_url = \"https://192.168.2.11:8006/api2/json\" pm_api_token_id = \"terraform@pam!terraform_token\" pm_api_token_secret = \"&lt;your-secret-here&gt;\" pm_tls_insecure = true}#Configure Proxmox Resources Hereresource \"proxmox_vm_qemu\" \"Kubernetes-Node\" { count = 4 name = \"Kubernetes-Node-${count.index + 1}\" vmid = var.vmid + (count.index * 100) target_node = var.proxmox_host clone = var.template_name #Basic VM settings here agent = 1 # QEMU Guest Agent, 1 means installing the guest agent on this VM is set to True os_type = \"cloud-init\" cores = 2 sockets = 1 cpu = \"host\" memory = 4096 scsihw = \"virtio-scsi-pci\" bootdisk = \"scsi0\" disk { slot = 0 # set disk size here. leave it small for testing because expanding the disk takes time. size = \"50G\" type = \"scsi\" storage = \"local-lvm\" iothread = 1 } # if you want two NICs, just copy this whole network section and duplicate it network { model = \"virtio\" bridge = \"vmbr0\" } lifecycle { ignore_changes = [ network, ] } # sshkeys set using variables. the variable contains the text of the key. sshkeys = &lt;&lt;EOF ${var.ssh_key} EOF}Things worth noting with the Terraform code above would be the following: count - I have this set to 4 but you can simply change this number depending on how many nodes you intend to deploy for your Kubernetes cluster. vmid - I have this starting at 100 from the vars.tf file with the default and then I multiply by 100 for every VM deployed after that. This means that my VMs will deployed with the IDs such as Node-1(100), Node-2(200), Node-3(300), etc.. This is just what I’ve grown accustomed to using, but choose whatever you like here. disk/size - When using Rancher, later we will deploy our cluster by using RKE which puts all the Kubernetes components in their own docker containers when deploying to the nodes. These images that get pulled down are not super small and they add up quickly. The last thing you want is to run out of storage on the nodes and wonder why it’s not working while all these processes are taking place in the background. Give your VMs a healthy amount of headroom for what is to come.vars.tfvariable \"ssh_key\" { default = \"&lt;your-ssh-key&gt;\"}variable \"proxmox_host\" { default = \"ProxMoxK8S\"}variable \"template_name\" { default = \"KubernetesNodeTemplate\"}variable \"vmid\" { type = number default = 100 }Set-up Update Upgrade Install DockerLog onto the server via SSH, update and upgrade, and finally install docker. For rancher, a specific version of docker seems to be needed. Use the following script to install docker:curl https://releases.rancher.com/install-docker/20.10.sh | shSet up Rancher K3sTime to set up your Rancher K3s cluster. This “cluster” (it really isn’t) is the management cluster for your kubernetes cluster. The setup process is easy.Run the following command on your GUI node:sudo docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancherThis command will start rancher in a docker container on your GUI node.After the docker container is up, you can hit the internal IP of the GUI node at https://&lt;INTERNAL_IP&gt;:443You will be welcomed by a screen asking you to create a password. run the command show and create an admin password.After you have signed in, you will be greeted by the Rancher Dashboard.Create your Kubernetes ClusterNow it’s time to create your cluster.Click the “Create button.Click the “Custom” option.Name your clusterKeep everything default except for the Nginx Ingress option under advanced options. Set this to Disabled. We are going to install traefik later.Click “Next”.Join your master and worker nodes to the clusterNow it’s time to add your master and worker nodes to your new cluster.To register your master node, make sure these boxes are checked (etcd, Control Plane). Click on the command to copy to your clipboard and paste it into the terminal with your SSH session to the master node. (You can use your master node as a worker node as well. I would not recommend this.To register your master node, make sure this box is checked (Worker). Click on the command to copy to your clipboard and paste it into the terminal with your SSH session to the worker node.Complete" }, { "title": "Building a Search Feature in a Python Flask App", "url": "/posts/FlaskAppWithSimpleSearch/", "categories": "Programming, Python", "tags": "python, flask", "date": "2022-12-20 09:00:00 -0800", "snippet": "Going through the steps on how to build a search feature in a Python Flask app. Flask is a popular microweb framework that makes it easy to build web applications in Python. We will use Jinja2 templates to create the user interface and a search function to retrieve search results from a database.Table of Contents Creating a Flask App with Search Feature Creating Search Template Optional CSS for Search Creating the Results Template Implementing Search Function to use a Database Creating the Database Conclusion Completed FlaskApp.py Completed Search HTML Template Completed Results HTML Template It Works! Creating a Flask App with Search Featurefrom flask import Flask, request, render_templateapp = Flask(__name__)@app.route('/')def search_page(): return render_template('search.html')@app.route('/search', methods=['POST'])def search(): query = request.form['query'] # Perform search and return results return render_template('results.html', query=query, results=results)if __name__ == '__main__': app.run()This Flask app has two routes: / and /search. The / route renders the search.html template, which should include a form with a search bar. The /search route handles the form submission and performs the search using the query entered by the user. The search results are then passed to the results.html template, which can display the results in a list or table format.To use this app, we will need to create the search.html and results.html templates and place them in the templates directory. We will also need to implement the search function and define the results variable.Creating Search TemplateWe need to create a template. To do this, we need create a folder called templates in the root directory of the flask app. Our directory should look like this:$ tree.└── FlaskAppProjectFolder └── templatesLet’s start by creating a simple HTML landing page for our search bar called search.html and save it into the newly created templates folder with the following code:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Search Page&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;Search&lt;/h1&gt; &lt;form action=\"/search\" method=\"POST\"&gt; &lt;input type=\"text\" name=\"query\" placeholder=\"Enter search query\"&gt; &lt;button type=\"submit\"&gt;Search&lt;/button&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt;Now our directory should look like this:$ tree.└── FlaskAppProjectFolder └── templates └── search.htmlIf you open that HTML file in a browser of your choice, you will see the following:This template includes a simple form with a text input field for the search query and a submit button. The form’s action is set to /search, which is the URL that the form will submit the search query to. The method is set to POST, which means that the search query will be sent to the server as part of the HTTP request body.You can customize this template further by adding additional form fields or styling the form with CSS. You can also use Flask’s template inheritance feature to create a base template that includes the search form, and then extend the base template in other templates as needed.Optional CSS for Search&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Search Page&lt;/title&gt; &lt;style&gt; form { width: 500px; margin: 0 auto; text-align: center; background-color: #eee; padding: 20px; border-radius: 5px; } input[type=\"text\"] { width: 70%; padding: 12px 20px; margin: 8px 0; box-sizing: border-box; border: 2px solid #ccc; border-radius: 4px; } button[type=\"submit\"] { width: 20%; background-color: #4CAF50; color: white; padding: 14px 20px; margin: 8px 0; border: none; border-radius: 4px; cursor: pointer; } button[type=\"submit\"]:hover { background-color: #45a049; } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;Search&lt;/h1&gt; &lt;form action=\"/search\" method=\"POST\"&gt; &lt;input type=\"text\" name=\"query\" placeholder=\"Enter search query\"&gt; &lt;button type=\"submit\"&gt;Search&lt;/button&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt;This template includes some basic CSS styles to make the form look more visually appealing. The form has a fixed width, a centered layout, and a light grey background color. The input field and submit button have some basic styling applied to them, including a border, padding, and a hover effect for the submit button.This is what that results of the CSS styling looks like when opening this HTML file in a browser:Creating the Results TemplateNext let’s create a search results template with the filename results.html in the templates folder using the following code:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Search Results&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;Results for: &lt;/h1&gt; &lt;ul&gt; &lt;/ul&gt;&lt;/body&gt;&lt;/html&gt;This template uses Jinja2 syntax to display the search results in an unordered list. The query and results variables are passed to the template from the search route in the Flask app.Now our directory should look like this:$ tree.└── FlaskAppProjectFolder └── templates ├── search.html └── results.htmlImplementing Search Function to use a DatabaseNow that we have a base flask app with our search and results templates created in the templates folder, we can add in the following block of code (After app = Flask(__name__) and Before def perform_search(query):) to implement our search function:import sqlite3def perform_search(query): # Connect to the database conn = sqlite3.connect('database') c = conn.cursor() # Execute the search query c.execute(\"SELECT * FROM table WHERE column LIKE ?\", (query,)) results = c.fetchall() # Close the connection conn.close() return results When using sqlite3.connect() as shown here in the code block. I have seen database and database.db passed here. If you getting the following error: sqlite3.DatabaseError: file is not a database then it will likely be resolved by dropping the .db file extension.This search function connects to a SQLite database, executes a search query that matches the query string against a column in a table, and returns the search results. You can customize the search query to meet the specific needs of your database and search requirements.You will need to make sure that you have a database file named database (or database.db) in the same directory as your Flask app, and that you have a table with a column that you want to search. You will also need to install the sqlite3 library in order to use it in your Flask app because now we are importing sqlite3 at the beggining of our code.Creating the DatabaseFor this Flask app we will us the following database as an example:CREATE TABLE users ( id INTEGER PRIMARY KEY, username TEXT, email TEXT);INSERT INTO users (username, email) VALUES (\"user1\", \"user1@example.com\"), (\"user2\", \"user2@example.com\"), (\"user3\", \"user3@example.com\");This database file contains a table named users with three columns: id, username, and email. It also includes three rows of data for three sample users.This database file contains only the CREATE TABLE and INSERT INTO statements, which define the structure of the users table and insert data into the table. However, these statements do not create the actual database file.To create the database file, you will need to execute these statements using a SQLite client or by using the execute() method in your Flask app. For this example we will use the execute() method in our Flask app to create the database file and populate the users table:import sqlite3def create_database(): # Connect to the database conn = sqlite3.connect('database.db') c = conn.cursor() # Create the users table c.execute( '''CREATE TABLE users ( id INTEGER PRIMARY KEY, username TEXT, email TEXT )''' ) # Insert data into the users table c.execute( '''INSERT INTO users (username, email) VALUES (\"user1\", \"user1@example.com\"), (\"user2\", \"user2@example.com\"), (\"user3\", \"user3@example.com\")''' ) # Save changes and close the connection conn.commit() conn.close()create_database()This code creates the database file for us and executes the CREATE TABLE and INSERT INTO statements to create the users table and insert data into the table.Place this block after app = Flask(__name__) and before our recently created def perform_search(query): lines.ConclusionTo recap, we have created a FlaskApp.py. Added a search function to use with a database. Added some sample data with the execute() method to create that database. Added our search and results templates with working routes to be used in our URLs and serve up the pages.Our Flask app directory will look like the following:$ tree.└── FlaskAppProjectFolder ├── FlaskApp.py ├── database └── templates ├── search.html └── results.htmlCompleted FlaskApp.pyfrom flask import Flask, request, render_templateimport sqlite3app = Flask(__name__)def create_database(): # Connect to the database conn = sqlite3.connect('database') c = conn.cursor() # Create the users table c.execute( '''CREATE TABLE users ( id INTEGER PRIMARY KEY, username TEXT, email TEXT )''' ) # Insert data into the users table c.execute( '''INSERT INTO users (username, email) VALUES (\"user1\", \"user1@example.com\"), (\"user2\", \"user2@example.com\"), (\"user3\", \"user3@example.com\")''' ) # Save changes and close the connection conn.commit() conn.close()create_database()def perform_search(query): # Connect to the database conn = sqlite3.connect('database') c = conn.cursor() # Execute the search query c.execute(\"SELECT * FROM users WHERE username LIKE ?\", (query,)) results = c.fetchall() # Close the connection conn.close() return results@app.route('/')def search_page(): return render_template('search.html')@app.route('/search', methods=['POST'])def search(): query = request.form['query'] results = perform_search(query) return render_template('results.html', query=query, results=results)if __name__ == '__main__': app.run()Completed Search HTML Template&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Search Page&lt;/title&gt; &lt;style&gt; form { width: 500px; margin: 0 auto; text-align: center; background-color: #eee; padding: 20px; border-radius: 5px; } input[type=\"text\"] { width: 70%; padding: 12px 20px; margin: 8px 0; box-sizing: border-box; border: 2px solid #ccc; border-radius: 4px; } button[type=\"submit\"] { width: 20%; background-color: #4CAF50; color: white; padding: 14px 20px; margin: 8px 0; border: none; border-radius: 4px; cursor: pointer; } button[type=\"submit\"]:hover { background-color: #45a049; } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;Search&lt;/h1&gt; &lt;form action=\"/search\" method=\"POST\"&gt; &lt;input type=\"text\" name=\"query\" placeholder=\"Enter search query\"&gt; &lt;button type=\"submit\"&gt;Search&lt;/button&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt;Completed Results HTML Template&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Search Results&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;Results for: &lt;/h1&gt; &lt;ul&gt; &lt;/ul&gt;&lt;/body&gt;&lt;/html&gt;It WorksSearching for a userGetting results back from database for user1" }, { "title": "DevOps the GitLab Way", "url": "/posts/2022-DevOpsTheGitLabWay/", "categories": "CI/CD, GitLab", "tags": "gitlab, devops, ci, cd, docker", "date": "2022-10-11 10:00:00 -0700", "snippet": "Have you been searching for a platform to host your code, and build your DevOps workflow and processes all in one place? Why not use GitLab for CI/CD pipelines? GitLab lets you host your application code and use the GitLab CI CD feature to continuously test, build and deploy your application on every code change.Table of Contents Prerequisites Initializing a GitLab Project Configuring Jobs on GitLab Securing Credentials by Creating Secret Variables Building and Pushing a Docker Image to Docker Hub Configuring Stages on the GitLab CI CD pipelinePrerequisites A GitLab account, a repository already set up, and a personal access token. A Docker Hub account and a repository.Initializing a GitLab ProjectBefore configuring a CI/CD pipeline on GitLab, you must first have a repository on GitLab that houses your application. This tutorial uses a Python flask application to illustrate GitLab CI CD in action.Clone the following repo:git clone https://gitea.ochoaprojects.com/brandon/gitlab-devop-scripts.gitOnce cloned, open the directory containing your files from your root directory and open it up with your preferred code editor.Next, run the command below to spin up flask run the flask application.flask runThe output below indicates that the flask application is entirely functional and error-free.Lastly, push the application up to your GitLab repository.Configuring Jobs on GitLabAfter initializing your GitLab project, you’ll need a way to execute tasks on your CI/CD pipeline. How? By configuring Jobs where you define tasks in a YAML file.You’ll write the CI/CD pipeline as code in a YAML file, which GitLab will use to perform the CI/CD pipeline for your project.There are two options involved when creating the YAML file: Create, and push the file in your project directory locally from your machine alongside the pipeline configurations to your remote GitLab repository. Create the file directly on your GitLab repository via the GitLab UI.In this example, you’ll create the YAML file directly on your GitLab repository via the GitLab UI:Click on the (+) icon shown below in your project’s root directory of your GitLab repository to initialize creating the (.gitlab-ci.yaml) file. Your browser redirects to a page where you can configure the new YAML file (step two).Next, type in .gitlab-ci.yml in the input field with the filename placeholder, as shown below.Add the code snippets below to the .gitlab-ci.yml file, which runs the test for the Python flask application.run_tests: # Use GitLab-managed runner with a python:3.9-slim-buster image to run the job image: python:3.9-slim-buster before_script: # Update the system package index - apt-get update # Install Python3 and pip - apt-get install -y python3-dev python3-pip # Install Flask and pytest - pip3 install Flask pytest script: # Run the pytest command to run the test for the Python flask application. - pytestNow, scroll down to the page’s bottom, and click on the Commit changes button to commit the changes. Doing so makes GitLab detect the pipeline configuration code and run the job.Lastly, navigate to the CI/CD section (left panel), and click on the Pipelines tab to view your pipeline.You’ll see that the pipeline is currently in running status, as shown below.Once executed successfully, your pipeline’s status changes to passed, which indicates your job ran successfully.Securing Credentials by Creating Secret VariablesYou’ve just successfully executed your first job. And now, you’re almost ready to build and push the Docker image for the Python flask project to both Docker Hub and GitLab container registries.But first, you’ll need a way to store your login credentials for Docker Hub and GitLab securely. What’s that secure way? You’ll create secret variables to hold these credentials and keep them away from public eyes.Navigate to Settings (left panel), and click on the CI/CD tab, as shown below, to create a secret variable using GitLab project variables.Next, scroll down on the new page, click on the Variables sub-section, and click Expand (top-right of the section) to expand the Variables section.Once expanded, click Add variable (bottom-left) to add a secret variable.Now, configure the new secret variable with the following: Key – DOCKER_USERNAME Value – Input your Docker Hub username.Finally, repeat step four, and add more variables with the following details: Key – DOCKER_PASSWORD Value – Provide your Docker Hub password. Key - GITLAB_USERNAME Value - Input your GitLab username. Key – GITLAB_ACCESS_TOKEN Value – Input your GitLab access token.You should now have four secret variables, as shown below.Building and Pushing a Docker Image to Docker HubWith your secrets kept a secret, it’s time to configure your second job. This job will build and deploy a Docker image for the Python flask application to Docker Hub.Navigate to the CI/CD Pipeline Editor as you’ll need to add configurations to the pipeline.Next, add the following code at the top of the run-test block. Be sure to replace dockerhub-user with your Docker Hub username.variables: IMAGE_NAME: &lt;dockerhub-user&gt;/flask-app IMAGE_TAG: flask-app-1.0Add the following code below the run_test block, which builds a Docker image for the Python flask application. Be sure to replace gitlab-user with your GitLab username and project-name (in this example, mine is “gitlab-devop-scripts”) in the script block.build_image: # Downloads a Docker client (docker:20.10.16). image: docker:20.10.16 # Connects to a Docker daemon. services: - docker:20.10.16-dind # Create a certificate directory (DOCKER_TLS_CERTDIR) to enable the # Docker client and Docker daemon to share the same certificate directory. # Enable Docker client and Docker daemon to authenticate each other. variables: DOCKER_TLS_CERTDIR: '/certs' # Authenticate GitLab with login parameters described as secret variables. before_script: - docker login -u $DOCKER_USERNAME -p $DOCKER_PASSWORD - docker login registry.gitlab.com -u $GITLAB_USERNAME -p $GITLAB_ACCESS_TOKEN # Build and push the flask image to DockerHub and GitLab container registry. script: - docker build -t $IMAGE_NAME:$IMAGE_TAG . - docker push $IMAGE_NAME:$IMAGE_TAG - docker build -t registry.gitlab.com/gitlab-user/project-name . - docker push registry.gitlab.com/gitlab-user/project-nameNow, click on Commit changes again to save the changes, and trigger the updated pipeline.You should now have the following output, which indicates your build is successful.We should also now see that our container registry push was successful by navigating to Package and registries &gt; Container Registry and the following output will be shown below.Navigate to the Packages &amp; Registries (left panel) → Container Registry to confirm your Python flask image has been pushed to the GitLab container registry, as shown below.Finally, switch to your Docker account and verify your Python flask application has been pushed to your Docker Hub registry.Configuring Stages on the GitLab CI CD pipelineCurrently, each job in your pipeline is executed in isolation and running regardless of if any job is not successful, which is not good practice.The best way to configure a pipeline is to make jobs execute accordingly, one after the other. With this behavior, if one job fails, other subsequent jobs will not execute.Navigate to the Pipeline Editor and add the following code below the variables block in your pipeline.The code below adds stages to the pipeline, referenced from jobs in the pipeline.stages: - test - buildReference the stages in both the run_test and build_image blocks, as shown below.Lastly, commit your changes to the pipeline, and navigate to Pipelines under the CI/CD section (left panel).If all goes well, you’ll see each job executed one after the other in the Stages column, as shown below.Once the run_tests job is done executing successfully, the build job begins immediately.When the build job is executed successfully, you’ll see the following output showing two green checks under the Stages column." }, { "title": "Session Manager for AWS", "url": "/posts/SessionManagerAWS/", "categories": "Cloud Providers, Amazon Web Services (AWS)", "tags": "aws, amazon, session manager, ssh tunnel, iam, ec2", "date": "2022-10-04 10:00:00 -0700", "snippet": "Session Manager is the useful AWS tool that you might not be thinking about. If you’ve ever run something like ProxMox you understand just how handy it is to be able to click “console” for any of your VMs and instantly be transported to that VM as if you were sitting directly infront of the real deal with ease. This is “almost” that, but with a little more power in my opinon.Table of Contents Create IAM Role Launch EC2 Instance Configure CloudWatch Install AWS CLI and AWS Session Manager Plugin Create IAM Policy and IAM User and IAM Group IAM Policy IAM Group IAM User SSH to EC2 Instance Port Forward from EC2 to localhost Setting Up EC2 Instance Starting Port Forward The Session Manager is part of the AWS Systems Manager and if you want to know more or read up on the offical documentation, that can be found here.Create IAM RoleWhen you create an AWS account, you begin with one sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account. It is strongly recommended that you do not use the root user for your everyday tasks. Safeguard your root user credentials and use them to perform the tasks that only the root user can perform.In this procedure, you use the AWS account root user to create your first user in AWS Identity and Access Management (IAM). You add this IAM user to an Administrators group, to ensure that you have access to all services and their resources in your account. The next time that you access your AWS account, you should sign in with the credentials for this IAM user. As a best practice, create only the credentials that the user needs. For example, for a user who requires access only through the AWS Management Console, do not create access keys.Click Create Role.Select AWS Service and choose EC2 at the bottom.Search for “AmazonSSMFullAccess”, tick the box, click next. Give your role a name, review, and create it.Launch EC2 InstanceAWS Systems Manager Agent (SSM Agent) is Amazon software that can be installed and configured on an EC2 instance, an on-premises server, or a virtual machine (VM). SSM Agent makes it possible for Systems Manager to update, manage, and configure these resources.If the Amazon Machine Image (AMI) type you choose in the first procedure doesn’t come with SSM Agent preinstalled, manually install the agent on the new instance before it can be used with Systems Manager. If SSM Agent isn’t installed on the existing EC2 instance you choose in the second procedure, manually install the agent on the instance before it can be used with Systems Manager.In most cases, SSM Agent is preinstalled on AMIs provided by AWS for the following operating systems (OSs): Amazon Linux Base AMIs dated 2017.09 and later Amazon Linux 2 Amazon Linux 2 ECS-Optimized Base AMIs Amazon EKS-Optimized Amazon Linux AMIs macOS 10.14.x (Mojave), 10.15.x (Catalina), and 11.x (Big Sur) SUSE Linux Enterprise Server (SLES) 12 and 15 Ubuntu Server 16.04, 18.04, and 20.04 Windows Server 2008-2012 R2 AMIs published in November 2016 or later Windows Server 2016, 2019, and 2022Navigate to EC2 &gt; Instances and click “Launch Instances” in the top right hand corner and name your EC2 whatever you want.The defaults here should be good and keep you in a “Free Tier” for learning purposes. However, the most important part here is the Advanced details at the bottom of this page. Expanding this will reveal the IAM instance profile option. Click the drop down and choose the IAM Role we made earlier.Now we simply launch our instance.Since we are using the SessionManager to connect to this EC2 instance for this exmaple we are able to complete to proceed without using a key pair.With what we have so far, it should be possible for us to head over to AWS Systems Manager &gt; Session Manager within AWS console and click Start Session with the EC2 instance we just made.Configure CloudWatch What is CloudWatch?– Amazon CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), IT managers, and product owners. CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, and optimize resource utilization. CloudWatch collects monitoring and operational data in the form of logs, metrics, and events. You get a unified view of operational health and gain complete visibility of your AWS resources, applications, and services running on AWS and on-premises. You can use CloudWatch to detect anomalous behavior in your environments, set alarms, visualize logs and metrics side by side, take automated actions, troubleshoot issues, and discover insights to keep your applications running smoothly.This is an optional step but it is also very powerful. This will allow you to output your sessions as logs into CloudWatch so that you can review them later.Start by going to Cloud Watch &gt; Log Groups in the console and Create log group.Give it whatever name you prefer, in this example I am going with “ssm-session”. Click “Create”Now we need session manager to use the log we just created. Navigate to AWS Systems Manager &gt; Session Manager and click on Preferences then Edit. I disable “Enforce Encryption” for the sake of simplicity in this example.First we are going to Enable CloudWatch Logging. Next we are going to keep the Recommended default of Stream Session Logs. Then we are going to uncheck the Enforce Encryption for this log group. Lastly we are going to choose the log group we just made in our CloudWatch in the last step.Install AWS CLI and AWS Session Manager PluginFor this example I am on Mac. However, this can be achieved with both Windows and Linux. My recommendation for windows is to use Chocolately. When using Linux you can of course use your native package manager such as RPM or APT.While using Mac I try to leverage Homebrew when I can.Here are the two commands that will get AWS CLI and AWS Session Manager Plugin installed on your machine for Mac.brew install awsclibrew install --cask session-manager-pluginCreate IAM Policy and IAM User and IAM GroupIAM PolicyTo create an IAM Policy we navigate to IAMPolicies and click “Create Policy”.Choose the JSON tab and paste in the code below.{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ssm:StartSession\" ], \"Resource\": [ \"arn:aws:ec2:&lt;region&gt;:&lt;account-id&gt;:instance/*\" ], \"Condition\": { \"StringLike\": { \"ssm:resourceTag/service\": [ \"proxy\" ] } } }, { \"Effect\": \"Allow\", \"Action\": [ \"ssm:StartSession\" ], \"Resource\": [ \"arn:aws:ssm:&lt;region&gt;::document/AWS-StartPortForwardingSession\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"ssm:TerminateSession\" ], \"Resource\": [ \"arn:aws:ssm:*:*:session/${aws:username}-*\" ] } ]} Note the “AWS-StartPortForwardingSession” part in the above JSON. Session Manager allows for SSH tunneling and it is enabled here. This feature can be very useful as demontrated in this picture.This command tells SSH to connect to instance as user ec2-user, open port 9999 on my local laptop, and forward everything from there to localhost:80 on the instance. When the tunnel is established, I can point my browser at http://localhost:9999 to connect to my private web server on port 80.Be sure to update &lt;region&gt; and &lt;account-id&gt; to the values required for your environment.Review and name your IAM Policy and click “Create Policy”. For this example I am giving the name “DemoSessionPolicy”.IAM GroupNext head over to IAM &gt; User groups and click “Create Group”.Name your IAM Group. In this example I am naming it “SSMUsers” because whatever user I add to this group will have access to the Session Manager and be able to use the features we add earlier such as SSH tunneling.Attach the IAM Policy that we made earlier to the IAM Group. Customer managed polcies, such as the one we made, will be listed at the top of the list for convenience. Click “Create Group” at the bottom when complete.IAM UserNext head over to IAM &gt; Users and click “Add User”.Name your user and choose Access Key - Programmatic access for AWS credential type. We choose this because this user does not need to access the console and only want to be access the AWS resources using SSM through things such as the AWS CLI. Click “Next: Permissions” at the bottom when complete.Since we already have a group set up from the previous step, we can simply add our new user to the group we just created in this step by ensuring Add user to group is selected, checking the box next to the group we want to be added to, and then clicking “Next: Tags” at the bottom.I don’t add any tags for this exmaple. Simply click “Next: Review”.Click “Create User”. IMPORTANT! AWS Console will immediately provide us the Access key ID and Secret access key on the next screen because we chose the “Programmatic” access for this user. It is very important that you document this information somewhere safe that you know you will be able to access and find again. There is a Download .csv option here which is a useful quick and easy step. However, I recommend using something like the AWS Secrets Manager for storing information such as this (This feature is not free and will cost a small fee). If you close this screen without saving the information, you will *NOT* be able to retrieve this information again and you *WILL* need to recreate a new user to recreate the information.Document your Access key ID and Secret access key someplace safe and where you can find it again.SSH to EC2 InstanceNow that we have an IAM User, Role, Group, Policy set up and AWS CLI plus Session Manager Plugin installed we can ssh into our EC2 resource with the AWS CLI.First we need to configure our AWS CLI witht the following command.aws configureNext we will provide that AWS Access Key ID and AWS Secret Access Key that we definitely documented earlier.Afterwards it will ask for the following-Default region name: us-east-2 (This is my setting)Default output format: json (This is default and what I used)To start a session run the following command. Replace &lt;instance-id&gt; with your own instance-id.aws ssm start-session --target &lt;instance-id&gt;Port Forward from EC2 to localhostSetting Up EC2 InstanceTo demonstrate the port forwarding feature we will use a simple NGINX server on our demo ec2 instance.Let’s start by installing NGINX with the linux package manager by using the following command.sudo amazon-linux-extras install nginx1 -yI chose the Linux AMI provided by Amazon and therefore I am not using something such as yum or apt. Instead it uses “amazon-linux-extras”.After NGINX is installed, let’s us systemd to start and check our server’s status.sudo systemctl start nginxsudo systemctl status nginxNow we will see that our NGINX server is active and running.Starting Port ForwardLet’s start a port forward session with the following command. Be sure to enter the correct ec2-instance-id for your command.aws ssm start-session \\ --target &lt;ec2-instace-id&gt; \\ --document-name AWS-StartPortForwardingSession \\ --parameters '{\"portNumber\":[\"80\"], \"localPortNumber\":[\"8080\"]}'Your terminal will display that it is Waiting for connections...Let’s make that connection! Simply open your browser from your local machine that you are running the AWS SSM command from and navigate to http://localhost:8080 and you will be greeted with “Welcome to NGINX on Amazon Linux!”We have successfully created and used our SSH Tunnel with the SSM!" }, { "title": "Using AWS Lambda and API Gateway to Check URLs", "url": "/posts/UsingAWSLambdaWithAPIGateway/", "categories": "Programming, Python", "tags": "terraform, python, aws, automation, api gateway, lambda, cloudwatch", "date": "2022-07-25 10:00:00 -0700", "snippet": "Using AWS Lambda Python functions and an API Gateway to deploy a URL Checker that looks for known malicious websites all while being serveless through Amazon Web Services.Table of Contents Overview DynamoDB dynamodb.tf AWS Lambda Functions getMalwareCheck.py postMalwareCheck.py lambda.tf dynamodbpolicy.json Lambda Layer To Create a Lambda Layer API Gateway apigateway.tf CloudWatch TestOverviewThe goal here is to setup some AWS infrastructure so that we can send GET requests to an API and have it trigger our AWS Lambda function written in python to check against a database of URLs that give us response one whether or not the URL we provided is in fact a known malicious site. Additionally there will be a functionality to use the same API Gateway to POST new URLs to the database through a seperate Lambda function by using a seperate route. In the end we want to deploy this all automatically through just Terraform.The flow will look like this:DynamoDBI went with DynamoDB as it is a fully managed, serverless, NoSQL database. DynamoDB offers built-in security, continuous backups, automated multi-Region replication, in-memory caching, and data export tools. Because the database backend wasn’t very complex and didn’t require multiple tables, but could grow to thousands of URLs, I wanted a non-relational database.I started by building the DynamoDB using Terraform. I did this manually using the aws_dynamodb_table resource, but ran into a few issues. Eventually I switched to using the dynamodb-table AWS module from Anton Babenko for speed’s sake and a clean deploy.I went with a Provisioned billing mode over On-Demand, as it was easy to limit the read/write capacity for this proof of concept, but it could easily be increased.The database had a very simple attribute setup:attributes = [ { name = \"BlacklistURL\" type = \"S\" }This sets up a way to store URLs as strings in “BlacklistURL”. Once I had the database built, I tested it by going to the AWS Console DynamoDB section, and manually adding a few items:From the DynamoDB menus under Tables &gt; Explore Items, I can see the items being successfully added:dynamodb.tf# --- dynamodb.tf ---module \"dynamodb_table\" { source = \"terraform-aws-modules/dynamodb-table/aws\" name = \"MalwareBlacklist\" hash_key = \"BlacklistURL\" billing_mode = \"PROVISIONED\" read_capacity = 10 write_capacity = 10 attributes = [ { name = \"BlacklistURL\" type = \"S\" } ] tags = { Terraform = \"true\" }}AWS Lambda FunctionsNow that the database was set up and available in AWS, I had to start coding my Lambda functions. I decided to use Python, and I was aware of a few modules I could use to simplify my coding.Boto3 is the AWS SDK for Python. It allows you to directly create, update, and delete AWS resources from your Python scripts. Now while I will be using Terraform for handling all of the infrastructure, this did give an easy way to interact with DynamoDB.Although I could have used one Python script and had different behavior based on the event, I decided to have two separate Lambda functions to make the routing of the requests more logically separate. I started by building the getMalwareCheck.py function to read from the database and check for the URL, and then did the postMalwareCheck.py to add new values to the database.I set up the client connection to the database, specified the table, and then set up a variable for the input to the function. I used the validators Python library to do a quick check on the input to make sure it was a proper URL.Once the Python was ready, I was able to run it locally, providing hardcoded inputs, and verify if I was able to get results from the table properly, or add to the table; checking in the AWS Console.Once everything checked out, I tested running the Lambda functions locally using the python-lambda-local project. I wrote some JSON files with the inputs that the functions would expect to get to test with as input.Once that worked, I wrote the Terraform to build the Lambda functions. A very cool method I found is that you can specify a data object, that would auto-build the Python code as a zip file:data \"archive_file\" \"malware_post_zip\" { type = \"zip\" source_file = \"../python/postMalwareCheck.py\" output_path = \"../python/postmalwarecheck.zip\" Note: that this only works in simple cases. If the deployment package is larger than 50MB, direct upload isn’t possible. Typically you would need to upload the zip file to an S3 bucket then call it to the Lambda function from there.getMalwareCheck.py#!/usr/bin/python3import osimport boto3import jsonimport validatorsfrom boto3.dynamodb.conditions import Key# Lambda Handler - cannot modify this methoddef lambda_handler(event, context): # Logging for CloudWatch print(event) dynamodb_client = boto3.client('dynamodb') dynamodb = boto3.resource('dynamodb') table = dynamodb.Table('MalwareBlacklist') MalwareURL = event['queryStringParameters']['MalwareURL'] try: # Validate that the input is a proper URL. Raise exception if it is not. validurl=validators.url(MalwareURL) if validurl==True: # Query DynamoDB for the MalwareURL: response = table.query(KeyConditionExpression=Key('BlacklistURL').eq(MalwareURL)) print(response) # If the URL exists in the table, the ScannedCount will be &gt; 0 if response['Count'] &gt; 0: return { 'statusCode': 200, 'body': json.dumps('URL is present in blacklist!') } elif response['Count'] == 0: return { 'statusCode': 200, 'body': json.dumps('URL is not found in blacklist. Proceed.') } else: return { 'statusCode': 400, 'body': json.dumps('We have problem with the count check') } raise Exception('We have a problem with the count check.') else: return { 'statusCode': 400, 'body': json.dumps('The URL is not valid.') } raise Exception('URL is not valid.') except Exception: return { 'statusCode': 400, 'body': json.dumps('There was a problem querying the blacklist.') }postMalwareCheck.pypostMalwareCheck.py#!/usr/bin/python3import osimport boto3import jsonimport validatorsfrom boto3.dynamodb.conditions import Key# Lambda Handler - cannot modify this methoddef lambda_handler(event, context): # Logging for CloudWatch print(event) dynamodb = boto3.resource('dynamodb') client = boto3.client('dynamodb') table = dynamodb.Table('MalwareBlacklist') MalwareURL = event['queryStringParameters']['MalwareURL'] # Putting a try/catch to log to the user when an error occurs posting the URL, try: # Validate if this is a valid URL. Raise exception if it is not, valid=validators.url(MalwareURL) if valid==True: table.put_item( Item={ 'BlacklistURL': MalwareURL } ) return { 'statusCode': 200, 'body': json.dumps('New Malware URL Added to Blacklist Successfully!') } else: raise except: return { 'statusCode': 400, 'body': json.dumps('Error adding new Malware URL to Blacklist.') }lambda.tf# --- lambda.tf ---data \"aws_iam_policy_document\" \"lambda_assume_role_policy\" { statement { effect = \"Allow\" actions = [\"sts:AssumeRole\"] principals { type = \"Service\" identifiers = [\"lambda.amazonaws.com\"] } }}resource \"aws_iam_role\" \"lambda_role\" { name = \"lambda-lambdarole\" assume_role_policy = data.aws_iam_policy_document.lambda_assume_role_policy.json inline_policy { name = \"allow_dynamodb\" policy = file(\"dynamodbpolicy.json\") }}resource \"aws_lambda_layer_version\" \"lambda_layer\" { filename = \"../lambdalayer/lambdalayer.zip\" layer_name = \"lambdalayer\" compatible_runtimes = [\"python3.9\"]}data \"archive_file\" \"malware_post_zip\" { type = \"zip\" source_file = \"../python/postMalwareCheck.py\" output_path = \"../python/postmalwarecheck.zip\"}resource \"aws_lambda_function\" \"malware_post_function\" { function_name = \"MalwarePostFunction\" filename = \"../python/postmalwarecheck.zip\" source_code_hash = data.archive_file.malware_post_zip.output_base64sha256 role = aws_iam_role.lambda_role.arn runtime = \"python3.9\" handler = \"postMalwareCheck.lambda_handler\" timeout = 10 layers = [aws_lambda_layer_version.lambda_layer.arn]}data \"archive_file\" \"malware_get_zip\" { type = \"zip\" source_file = \"../python/getMalwareCheck.py\" output_path = \"../python/getmalwarecheck.zip\"}resource \"aws_lambda_function\" \"malware_get_function\" { function_name = \"MalwareGetFunction\" filename = \"../python/getmalwarecheck.zip\" source_code_hash = data.archive_file.malware_get_zip.output_base64sha256 role = aws_iam_role.lambda_role.arn runtime = \"python3.9\" handler = \"getMalwareCheck.lambda_handler\" timeout = 10 layers = [aws_lambda_layer_version.lambda_layer.arn]}dynamodbpolicy.json{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"dynamodb:BatchGetItem\", \"dynamodb:GetItem\", \"dynamodb:Scan\", \"dynamodb:Query\", \"dynamodb:BatchWriteItem\", \"dynamodb:PutItem\", \"dynamodb:UpdateItem\", \"dynamodb:DeleteItem\" ], \"Resource\": \"arn:aws:dynamodb:*\" } ]}Lambda LayerOften times you see or hear of Python virtual environments that have all the dependencies for a particular Python program to run. In this case we are not on a local machine running our Python code, but we still need the proper environment for our dependencies to be available. We can accomplish this in AWS by using a Lambda Layer. With a Lambda Layer we are able to specify it easily in Terraform along with the Lambda functions, and could be reused for multiple functions. A Lambda layer is a .zip file archive that can contain additional code or other content. A layer can contain libraries, a custom runtime, data, or configuration files.To Create a Lambda Layer Create a directory where we want to install our requests package: mkdir -p layer/python/lib/python3.9/site-packages Run pip3 install &lt;package&gt; -t layer/python/lib/python3.9/site-packages/ Access the layer folder: cd layer Zip the python folder containing all of the required files: zip -r lambdalayer.zip * Copy the zip file to a location where it can be referenced by Terraform.Resulting Output for this Project: lambdalayer.zipOnce the Lambda Layer was added to the Terraform resources, I was able to test the Lambda functions successfully in the AWS console:API GatewayUsing an HTTP API instead of a REST API made the most sense for this project as it is more of a proof of concept and learning experience rather than a full fledged deployed piece of infrastructure. It is also simple and straight forward while being cheaper.Here is the main receipe for the API Gateway: Create the API Gateway itself. Create a Stage for deployment. In my case I used the $default stage, which was set to auto-deploy. Create two Routes for the HTTP GET or POST traffic to be picked up, and send to their respective integrations. Create two Integrations to pass the HTTP GET or POST traffic to be passed to the proper Lambda function. Add permissions so the API Gateway has permission to invoke the Lambda function.apigateway.tf# --- apigateway.tf ---resource \"aws_apigatewayv2_api\" \"lambdagateway\" { name = \"malware_http_gateway\" description = \"HTTP API Malware Check Gateway\" protocol_type = \"HTTP\"}resource \"aws_apigatewayv2_stage\" \"lambdastage\" { api_id = aws_apigatewayv2_api.lambdagateway.id name = \"$default\" auto_deploy = true access_log_settings { destination_arn = aws_cloudwatch_log_group.api_gw.arn format = jsonencode({ \"requestId\" : \"$context.requestId\", \"extendedRequestId\" : \"$context.extendedRequestId\", \"ip\" : \"$context.identity.sourceIp\", \"caller\" : \"$context.identity.caller\", \"user\" : \"$context.identity.user\", \"requestTime\" : \"$context.requestTime\", \"httpMethod\" : \"$context.httpMethod\", \"resourcePath\" : \"$context.resourcePath\", \"status\" : \"$context.status\", \"protocol\" : \"$context.protocol\", \"responseLength\" : \"$context.responseLength\", \"integrationErrorMessage\" : \"$context.integrationErrorMessage\", \"errorMessage\" : \"$context.error.message\", \"errorResponseType\" : \"$context.error.responseType\" }) }}resource \"aws_apigatewayv2_route\" \"lambda_get_route\" { api_id = aws_apigatewayv2_api.lambdagateway.id route_key = \"GET /urlinfo/1\" target = \"integrations/${aws_apigatewayv2_integration.lambda_get_integration.id}\"}resource \"aws_apigatewayv2_integration\" \"lambda_get_integration\" { description = \"HTTP Integration HTTP GET to Lambda\" api_id = aws_apigatewayv2_api.lambdagateway.id integration_type = \"AWS_PROXY\" integration_method = \"POST\" integration_uri = aws_lambda_function.malware_get_function.invoke_arn passthrough_behavior = \"WHEN_NO_MATCH\"}resource \"aws_apigatewayv2_route\" \"lambda_post_route\" { api_id = aws_apigatewayv2_api.lambdagateway.id route_key = \"POST /addurl\" target = \"integrations/${aws_apigatewayv2_integration.lambda_post_integration.id}\"}resource \"aws_apigatewayv2_integration\" \"lambda_post_integration\" { description = \"HTTP Integration HTTP POST to Lambda\" api_id = aws_apigatewayv2_api.lambdagateway.id integration_type = \"AWS_PROXY\" integration_method = \"POST\" integration_uri = aws_lambda_function.malware_post_function.invoke_arn passthrough_behavior = \"WHEN_NO_MATCH\"}resource \"aws_lambda_permission\" \"api_gw_get\" { action = \"lambda:InvokeFunction\" function_name = aws_lambda_function.malware_get_function.function_name principal = \"apigateway.amazonaws.com\" source_arn = \"${aws_apigatewayv2_api.lambdagateway.execution_arn}/*/*/*\"}resource \"aws_lambda_permission\" \"api_gw_post\" { action = \"lambda:InvokeFunction\" function_name = aws_lambda_function.malware_post_function.function_name principal = \"apigateway.amazonaws.com\" source_arn = \"${aws_apigatewayv2_api.lambdagateway.execution_arn}/*/*/*\"}CloudWatchAlthough CloudWatch was not part of the original architecture, it was extremely valuable when troubleshooting and debugging. CloudWatch allows you to publish log groups from the various components to one place, making it easier to diagnose failures.The first thing was to create the log groups for each component. I was mainly concerned with getting some data on what was coming in to the HTTP API Gateway, and what was coming in to the Lambda functions. I set up log groups for each of these, and a policy to allow give permissions for logging to CloudWatch.# --- cloudwatch.tf ---resource \"aws_cloudwatch_log_group\" \"api_gw\" { name = \"/aws/api_gw/${aws_apigatewayv2_api.lambdagateway.name}\" retention_in_days = 14}resource \"aws_cloudwatch_log_group\" \"lambda_get\" { name = \"/aws/lambda/${aws_lambda_function.malware_get_function.function_name}\" retention_in_days = 14}resource \"aws_cloudwatch_log_group\" \"lambda_post\" { name = \"/aws/lambda/${aws_lambda_function.malware_post_function.function_name}\" retention_in_days = 14}resource \"aws_iam_policy\" \"lambda_logging\" { name = \"lambda_logging\" path = \"/\" description = \"IAM policy for logging from a Lambda\" policy = &lt;&lt;EOF{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:PutLogEvents\" ], \"Resource\": \"arn:aws:logs:*:*:*\", \"Effect\": \"Allow\" } ]}EOF}resource \"aws_iam_role_policy_attachment\" \"lambda_logs\" { role = aws_iam_role.lambda_role.name policy_arn = aws_iam_policy.lambda_logging.arn}TestTo test the entire setup, I mainly used Postman to craft HTTP GET and POST messages.When testing with Postman, I could add the HTTP API Gateway Invoke URL, select the type of HTTP request, and add the MalwareURL as a parameter:" }, { "title": "Plex Automation", "url": "/posts/PlexAutomation/", "categories": "Homelab, Plex", "tags": "docker, containers, webui, management, monitoring, automation, api, plex, sonarr, radarr, jackett, ombi, deluge", "date": "2022-07-06 10:00:00 -0700", "snippet": "How to build a complete Plex server with full automation for accepting and processing plex requests without any manual work.Table of Contents The Stack Tips for managing your hard drives Optimizing for Ratios HDD vs SSD HDD Backups vs. Redundancy Installing mhddfs Mounting Multiple Drives as One Installing Docker Plex Config Deluge Config Jackett Config Sonarr &amp; Radarr Docker Config Ombi Config Start it up!The StackHere’s the stack we’ll be using. There will be a section describing the installation and configuration for each one of these.Docker lets us run and isolate each of our services into a container. Everything for each of these services will live in the container except the configuration files which will live on our host.Plex is a “client-server media player system”. There are a few alternatives, but I chose Plex here because they have a client available on nearly every platform.Deluge is going to act as our download client for our sonarr and radarr applications of the stack. It will be fed torrent files from jackett after it finds the best match based off criteria we will set.Jackett is a tool that Sonarr and Radarr use to search indexers and trackers for torrents.Sonarr is a tool for automating and managing your TV library. It automates the process of searching for torrents, downloading them then “moving” them to your library. It also checks RSS feeds to automatically download new shows as soon as they’re uploaded!Radarr Is a fork of Sonarr that does all the same stuff but for Movies.Ombi is a super simple web UI for sending requests to Radarr and Sonarr.Tips for managing your hard drivesHere are some tips for how to manage your hard drives and data.Optimizing for RatiosI use private trackers to get my torrents, and this means I need to maintain a ratio. If you aren’t familiar with this term, it basically means you should be uploading as much, if not more than you download.The best way I’ve found to do this is to mount your drives directly to the machine that handles your downloads. This means you can configure Sonarr and Radarr to create hardlinks when a torrent finishes. With this enabled, a reference to the data will exist in your media directory and in your torrent directory so Deluge can continue seeding everything you download.HDD vs SSDCan also be written as Space vs. Reliability and Speed. I’m a hoarder when it comes to media now, so I go with HDDs. This means I need to worry about my drives randomly dying.If you roll with SSDs, I envy you and you should skip this next section!HDD Backups vs. RedundancyHDDs are very prone to randomly dying and you need to prepare for this. You can either set up an array of drives and use something like RAID, but you usually end up losing some space and it’s not very easy to expand. I’m building my library from scratch so I want to be able to expand the space on my server as I download more media.So instead, I mount all of my drives and then use mhddfs to treat them as one file system, and what’s really cool is that it automatically fills up your drives in order.update: I still haven’t found a good way to keep backups of this data and am actually working on a way around this now.Installing mhddfsmhddfs contains a bug where you can occasionally run into a segfault error when mounting your drives. You’ll want to install it via the patch in this repo, but it’s a bit tricky.The docker installation method is outdated and doesn’t run so you’ll have to install fakeroot and run the command from the Readme.sudo apt-get install -y fakerootgit clone https://github.com/vdudouyt/mhddfs-nosegfault.git ~/Downloadscd ~/Downloads/mhddfs-nosegfaultfakeroot dpkg-buildpackageIt’ll give you trouble about signing, ignore it and install the .deb packagedpkg -i ~/Downloads/mhddfs_0.1.39+nosegfault2_amd64.debMounting Multiple Drives as Onemkdir /mnt/hdd1mkdir /mnt/hdd2mkdir /mnt/mediaTo get your drives to mount on boot we have to edit your /etc/fstab file. Add the following lines at the end of your file and replace the drive IDs with your own.UUID=fa605d83-106e-4143-bb20-deec7461f08c /mnt/hdd1 ext4 auto,nofail,noatime,rw,user 0 0UUID=31a25f49-0905-47b9-9f48-e6a5c3f59293 /mnt/hdd2 ext4 auto,nofail,noatime,rw,user 0 0mhddfs#/mnt/hdd1,/mnt/hdd2 /mnt/media fuse defaults,allow_other 0 0This mounts both drives on /mnt/hdd1 and /mnt/hdd2 and then mounts them together via mhddfs on /mnt/media. Now let’s set up our file system with a folder for torrents and a couple for our Libraries.mkdir /mnt/media/torrentsmkdir /mnt/media/Moviesmkdir /mnt/media/ShowsInstalling Dockersudo apt-get updatesudo apt install docker.iosudo systemctl start dockersudo curl -L \"https://github.com/docker/compose/releases/download/1.25.4/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-composeI also like to keep my configs in one easy place in case I want to transfer them anywhere, so let’s create a folder to hold our docker container configs and create our docker-compose.yml filemkdir ~/docker-servicestouch ~/docker-services/docker-compose.ymlAnd add this to your docker-compose file. We will be filling in the services in the coming steps. If you are confused on how to add the services or how your file should look, here is a good resource on docker-compose.---version: \"2\"services:Plex Configplex: image: linuxserver/plex:latest container_name: plex environment: - PUID=1000 - PGID=1000 - VERSION=docker volumes: - ~/docker-services/plex/config:/config - /mnt:/mnt ports: - 32400:32400 restart: unless-stoppedThis will start your Plex server on port 32400, and add the volumes /mnt and ~/docker-services/plex/config onto the container. If you are trying to move your current Plex configs over, run something like thismv /var/lib/plexmediaserver/* ~/docker-services/plex/config/Note that plex is looking for your config directory to contain a single directory Library. Look for that directory and copy it over.Deluge Configdeluge: image: linuxserver/deluge container_name: deluge environment: - PUID=1000 - PGID=1000 - TZ=America/Los_Angeles volumes: - ~/docker-services/deluge/config:/config - /mnt/media/deluge:/mnt/media/deluge - /mnt/media/PlexMediaLibrary:/mnt/media/PlexMediaLibrary ports: - 8112:8112 restart: unless-stoppedNotice how we mount our deluge drive on the container in the same location as the host, rather than something like /downloads (which is suggested over at linuxserver). This, plus the config below ensures Sonarr and Radarr send torrents to the right directory.Jackett Configjackett: image: linuxserver/jackett container_name: jackett environment: - PUID=1000 - PGID=1000 - TZ=America/Los_Angeles - RUN_OPTS=run options here #optional volumes: - ~/docker-services/jackett/config:/config - /mnt/media/torrents/completed:/downloads ports: - 9117:9117 restart: unless-stoppedThis is super basic and just boots your Jackett service on port 9117. Doesn’t need much else.Sonarr &amp; Radarr Docker Configsonarr: image: linuxserver/sonarr container_name: sonarr environment: - PUID=1000 - PGID=1000 - TZ=America/Los_Angeles - UMASK_SET=022 #optional volumes: - ~/docker-services/sonarr/config:/config - /mnt/media:/mnt/media ports: - 8989:8989 restart: unless-stopped radarr: image: linuxserver/radarr container_name: radarr environment: - PUID=1000 - PGID=1000 - TZ=America/Los_Angeles volumes: - ~/docker-services/radarr/config:/config - /mnt/media:/mnt/media ports: - 7878:7878 restart: unless-stoppedMake sure those PUID and GUID match the ID for your user and group… and make sure that user has read-write permissions for /mnt/media. Sonarr and Radarr are going to try to be creating folders and files in there when they copy or hard link files over.If you are running into issues, check the logs of the docker container or the logs in the web UI. It should tell you exactly where it’s having trouble. Then log into the user you set it to run as an attempt the same actions. See whats going on first hand.Ombi Configombi: image: linuxserver/ombi container_name: ombi environment: - PUID=1000 - PGID=1000 - TZ=America/Los_Angeles - BASE_URL=/ombi #optional volumes: - ~/docker-services/ombi/config:/config ports: - 3579:3579 restart: unless-stoppedThis will open Ombi on port 3579 but sadly they don’t have SSL support by default. I can create a post on adding LetsEncrypt to this setup.Start it up!Run this command to boot up all your services!cd ~/docker-servicesdocker-compose up -dYou now have these services running locally. Go to their web UI’s and configure everything.sonarr @ http://localhost:8989radarr @ http://localhost:7878jackett @ http://localhost:9117transmission @ http://localhost:9091plex @ http://localhost:32400There is some final configuration you will need to do that is out of the scope of this tutorial, but I can help if need be.You will have to Configure Jackett with your indexers Point Sonarr and Radarr to Jackett for indexers and Deluge as a download client Tell Sonarr and Radarr to download Movies and Shows folders you created above Tell Sonarr and Radarr to use Hardlinks instead of Copies in advanced settings Configure Ombi to use Sonarr and Radarr for requestsNow go invite your friends to your Plex server" }, { "title": "About Me and Homelab Topology", "url": "/posts/AboutMeAndTopology/", "categories": "About Me", "tags": "about me, topology", "date": "2022-07-05 10:00:00 -0700", "snippet": "Hi! My name is Brandon Ochoa!…tech enthusiast, night owl, full time student of life, coffee addict… _ _ _ | | (_) | | ___ ___| |__ ___ __ _ _ __ _ __ ___ _ ___ ___| |_ ___ / _ \\ / __| '_ \\ / _ \\ / _` | '_ \\| '__/ _ \\| |/ _ \\/ __| __/ __|| (_) | (__| | | | (_) | (_| | |_) | | | (_) | | __/ (__| |_\\__ \\ \\___/ \\___|_| |_|\\___/ \\__,_| .__/|_| \\___/| |\\___|\\___|\\__|___/ | | _/ | |_| |__/ I find myself in a situation that I’m sure most others can relate to. That situation being when you spend all day and all night working on some projects, developing some new skills, playing with that new-found software, language, server hardware, etc. The problem is, you’re just having too much fun with it and you just don’t stop hacking away at whatever it is you are doing. Sooner or later, you are more or less done with that project or whatever it is you are working on for now, and you move on to the next adventure in your tech curiosity. When this happens, you find yourself a year or more later and realize you are trying to remember that cool piece of code you used, or that setting you changed, or that command line tool just right for the job, which leaves you stuck scratching your brain trying to remember. It’s important to realize that documenting all the cool fun stuff you are working with is just as vital as that cool fun stuff itself. I’m aiming to correct this aspect of my past, current, and future tech adventures and share them all with everyone here!Homelab TopologyI’ve been homelabbing for years now. Over those years, I have accumulated plenty of details I want to share about the things I have done. However, I will leave the specifics of those details to my individual posts where I will drill into those topics. I would like to use this space, here on this page, as an opportunity to share the overall topology of my homelab in which I operate.ServicesOther than the Topology for my homelab, I wanted to also give another brief overview of what I believe to be another significant part of my infrastructure. This next topology describes how to publish services on the internet by using CloudFlare as a proxy service, custom DNS records, a purchased domain, and a reverse proxy service such as NGINX.Author NoteIf you have made it to the bottom of this page, then I want you to know that this page and the “About” tab is the same content. I realize that this is redundant, but it was important to me that this overview and “about me” was at least viewed by visitors, if anything." }, { "title": "ProxMox Setup and Installation", "url": "/posts/ProxMoxSetupInstall/", "categories": "Homelab, ProxMox", "tags": "proxmox", "date": "2022-07-02 10:00:00 -0700", "snippet": "Proxmox VE is an open-source server platform for enterprise virtualization. As a Debian-based Linux distribution, Proxmox uses a modified Ubuntu kernel to run multiple virtual machines and containers on a single server.Table of Contents Download Proxmox ISO Image Prepare Installation Medium Launch the Proxmox Installer Run Proxmox Removing Subscription Pop-Up on Log-In One-Line Command Manual Commands Create a VM Configure Proxmox Virtual Environment Start the VM at Boot Increase/Decrease Virtual Disk Size Enable NAT Networking Mode (Bonus) Enable Dark Mode!!! Install Uninstall Download Proxmox ISO ImageThe first step is to download the Proxmox VE ISO image.Navigate to the official Proxmox Downloads page and select Proxmox Virtual Environment.This takes you to the Proxmox Virtual Environment Archive that stores ISO images and official documentation. Select ISO Images to continue.At the time of writing, the latest version of the Proxmox VE ISO Installer is 7.1. If a newer version is available, it is listed at the top. Click Download and save the file.Prepare Installation MediumCopy the Proxmox ISO image on a CD/DVD or a USB flash drive. Although both options are possible, it is assumed that most systems won’t have an optical drive.Plug in the USB drive and copy the ISO image to the USB stick using the command line or a USB formatting utility (such as Etcher or Rufus).If you are working on Linux, the fastest way is to create a bootable USB is to run the command:dd bs=1M conv=fdatasync if=./proxmox-ve_*.iso of=/device/nameCopied!If needed, modify the file name and path in if=./proxmox-ve_*.iso and make sure to provide the correct USB device name in of=/device/name.To find the name of your USB stick, run the following command before and after plugging in the device:lsblkCompare the output. The additional entry in the second output is the name of the device.Launch the Proxmox InstallerMove to the server (machine) where you want to install Proxmox and plug in the USB device.While the server is booting up, access the boot menu by pressing the required keyboard key(s). Most commonly, they are either Esc, F2, F10, F11, or F12.Select the installation medium with the Proxmox ISO image and boot from it.Next, the Proxmox VE menu appears. Select Install Proxmox VE to start the standard installation.Read and accept the EULA to continue.Choose the target hard disk where you want to install Proxmox. Click Options to specify additional parameters, such as the filesystem. By default, it is set to ext4.Then, set the location, time zone, and keyboard layout. The installer autodetects most of these configurations.Create a strong password for your admin credentials, retype the password to confirm, and type in an email address for system administrator notifications.The final step in installing Proxmox is setting up the network configuration. Select the management interface, a hostname for the server, an available IP address, the default gateway, and a DNS server. During the installation process, use either an IPv4 or IPv6 address. To use both, modify the configuration after installing.The installer summarizes the selected options. After confirming everything is in order, press Install.After the installation is complete, remove the USB drive and reboot the system.Run ProxmoxOnce the installation is completed and the system rebooted itself, the Proxmox GRUB menu loads. Select Proxmox Virtual Environment GNU/Linux and press Enter.Next, the Proxmox VE welcome message appears. It includes an IP address which loads Proxmox. Navigate to that IP address in a web browser of your choice.After navigating to the required IP address, you will most likely see a warning message that the page is unsafe because Proxmox VE uses self-signed SSL certificates. Select to proceed to the Proxmox web management interface.To access the interface, log in as root and provide the password set when installing Proxmox.A dialogue box pops up saying there is no valid subscription for the server. Proxmox offers an add-on service you can subscribe to, which is optional. To ignore the message, click OK.Removing Subscription Pop-Up on Log-InOne-Line CommandRun this command in the terminal of the host machine running your ProxMox instance.sed -Ezi.bak \"s/(Ext.Msg.show\\(\\{\\s+title: gettext\\('No valid sub)/void\\(\\{ \\/\\/\\1/g\" /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js &amp;&amp; systemctl restart pveproxy.serviceManual CommandsBelow is the manual process of acheiving the same results as the command above in multi-command steps.Change to working directorycd /usr/share/javascript/proxmox-widget-toolkitMake a backupcp proxmoxlib.js proxmoxlib.js.bakEdit the filenano proxmoxlib.jsLocate the following code(Use ctrl+w in nano and search for “No valid subscription”)Ext.Msg.show({ title: gettext('No valid subscription'),Replace “Ext.Msg.show” with “void”void({ //Ext.Msg.show({ title: gettext('No valid subscription'),Restart the Proxmox web service (also be sure to clear your browser cache, depending on the browser you may need to open a new tab or restart the browser)systemctl restart pveproxy.serviceYou can quickly check if the change has been made:grep -n -B 1 'No valid sub' proxmoxlib.jsYou have three options to revert the changes: Manually edit proxmoxlib.js to undo the changes you made Restore the backup file you created from the proxmox-widget-toolkit directory:mv proxmoxlib.js.bak proxmoxlib.js Reinstall the proxmox-widget-toolkit package from the repository:apt-get install --reinstall proxmox-widget-toolkitCreate a VMNow that you logged in to the Proxmox web console, create a virtual machine.Before going through the steps to create a virtual machine, make sure you have ISO images for installation mediums. Move to the resource tree on the left side of your GUI.Select the server you are running and click on local (pve1). Select ISO Images from the menu and choose between uploading an image or downloading it from a URL.Once you have added an ISO image, move on to spinning up a virtual machine. Click the Create VM button located on the right side of the header in the GUI.Provide general information about the VM:Start by selecting the Node. If you are starting and have no nodes yet, Proxmox automatically selects node 1 (pve1).Provide a VM ID. Each resource has to have a unique ID.Finally, create a name for the VM. Use an easily identifiable name.Next, switch to the OS tab and select the ISO image you want for your VM. Define the Type of the OS and kernel Version. Click Next to continue.Modify system options (such as the Graphic card and SCSI controller) or leave the default settings.Then, configure any Hard Disk options you want the VM to have. Generally, you can leave all the default settings. However, if the physical server is using an SSD, enable the Discard option.The number of Cores the physical server has determines how many cores you can provide to the VM. The number of cores allocated also depends on the predicted workload.Next, choose how much Memory (MiB) you want to assign to the VM.Move on to the Network tab. It is recommended to separate the management interface and the VM network. For now, leave the default setting.After clicking Next, Proxmox loads the Confirm tab that summarizes the selected VM options. To start the VM immediately, check the box under the listed information or start the VM manually later. Click Finish to create the VM.See the newly created VM in the resource tree on the left side of the screen. Click on the VM to see its specifications and options.Configure Proxmox Virtual EnvironmentAfter creating a virtual machine, move on to configuring the environment.Start the VM at BootIf the Start at boot option is set to No, the VM does not automatically start after rebooting the server. This means you need to log in to the Proxmox interface and start the VM manually.To change the default setting, highlight the option and click the Edit button above.Check the box and click OK.Increase/Decrease Virtual Disk SizeThe simplest way to increase or decrease the virtual disk size of a VM is through the command line interface, which can be done online or offline.When increasing disk space, modify the partition table and file system inside the VM to update to the new size.When decreasing a VM’s disk space, make sure to back up any data you want to save and reduce the file system and partition inside the VM first.The main syntax for increasing/decreasing virtual disk size is:qm resize [virtual_machine_ID] [disk] [size]Copied!For instance, to add 10G to a virtio0 disk on a VM with the ID 100, run:qm resize 100 virtio0 +10GEnable NAT Networking ModeAs mentioned above, it’s a good idea to change the default bridge networking mode to prevent the Proxmox host and VMs being on the same network. To create a separate network, enable NAT networking mode.To do so, edit the Interfaces file. Open the command line and run:sudo nano /etc/network/interfacesThe file shows that vmbr0 is the default bridge network for Proxmox, as in the lines below:auto loiface lo inet loopbackiface eno1 inet manualauto vmbr0iface vmbr0 inet staticaddress 10.10.22.215netmask 255.255.255.0gateway 10.10.22.1bridge_ports eno1bridge_stp offbridge_fd 0Add the following content to the file:auto vmbr1iface vmbr1 inet staticaddress 10.10.10.1netmask 255.255.255.0bridge_ports nonebridge_stp offbridge_fd 0post-up echo 1 &gt; /proc/sys/net/ipv4/ip_forwardpost-up iptables -t nat -A POSTROUTING -s '10.10.10.0/24' -o vmbr0 -j MASQUERADEpost-down iptables -t nat -D POSTROUTING -s '10.10.10.0/24' -o vmbr0 -j MASQUERADESave and exit.Finally, bring up the the new interface with:sudo ifup vmbr1Next time you create a VM, vmbr0 and vmbr1 will be available for the bridge network option. Select vmbr1 to keep the VM on a separate network from Proxmox.(Bonus) Enable Dark Mode!!!Everything is dark, including the graphs, context menus and all in between! Eyes need not be fried.InstallThe installation is done via the CLI utility. Run the following commands on the PVE node serving the Web UI:~# wget https://raw.githubusercontent.com/Weilbyte/PVEDiscordDark/master/PVEDiscordDark.sh~# bash PVEDiscordDark.sh installOr this onelinerbash &lt;(curl -s https://raw.githubusercontent.com/Weilbyte/PVEDiscordDark/master/PVEDiscordDark.sh ) installUninstallTo uninstall the theme, simply run the utility with the uninstall command." }, { "title": "Automating AWS VPC and EC2 Instances using Terraform", "url": "/posts/TerraformWithAWS/", "categories": "Infrastructure as Code, Terraform", "tags": "terraform, ec2, aws, automation, vpc", "date": "2022-06-28 10:00:00 -0700", "snippet": "Terraform is a powerful tool that allows us to automate infrastructure in our cloud providers and much more. Using terraform to automate the building and configuration of our AWS enviornment to be in the exact state we want it to be is not only a huge time saver, but also ensures we are not making a mistake each time we deploy it.Table of Contents Starting our project and setting a provider Create a Key Pair Create VPC Create Internet Gateway Create a Subnet Create Custom Route Table Associate Subnet with Route Table Create Security Group to Allow Port Web Traffic and SSH Create a Network Interface with an IP in the Subnet Assign an Elastic IP to the Network Interface Create Debian Server then Install and Enable Apache2 Terraform Plan Terraform Apply Checking the Results Terraform Destory Completed main.tf File main.tf Starting our project and setting a providerTerraform uses .tf files. We will start off by creating our main.tf file.touch main.tf Note: Putting all code in main.tf is a good idea when you are getting started or writing an example code. In all other cases you will be better having several files split logically like this: main.tf - call modules, locals, and data sources to create all resources variables.tf - contains declarations of variables used in main.tf outputs.tf - contains outputs from the resources created in main.tf versions.tf - contains version requirements for Terraform and providers terraform.tfvars should not be used anywhere except composition. To learn more of the best practices visit the Terraform Best Practices WebsiteWe will need to setup up our provider on the machine we will be running our terraform code on. In this case it will be AWS. To set our provider we use the following block:provider \"aws\" { region = \"us-east-1\" access_key = \"&lt;YOUR-ACCESS-KEY&gt;\" secret_key = \"&lt;YOUR-SECRET-KEY&gt;\"} Danger: It is best to use a secret vault for credential based values instead of placing them directly into your .tf file like this. For the sake of this example, we will continue as isPlease choose the region of your choice. In this example I am using us-east-1. To obtain your access_key and secret_key from the AWS console, start by heading to the “Security credentials” from the top right hand corner after clicking your username.Scroll down to the Access Keys section of the page and click “Create access key”Once your access key has been created, make note of your “Access key ID”. Click on the “show secret” button and also make note of your “Secret access key”.Now would be a good time to initialize our provider on our machine with Terraform by using the following command:terraform initAfter running the init command, you will see the following output:Initializing the backend...Initializing provider plugins...- Finding latest version of hashicorp/aws...- Installing hashicorp/aws v4.20.1...- Installed hashicorp/aws v4.20.1 (signed by HashiCorp)Terraform has created a lock file .terraform.lock.hcl to record the providerselections it made above. Include this file in your version control repositoryso that Terraform can guarantee to make the same selections by default whenyou run \"terraform init\" in the future.Terraform has been successfully initialized!You may now begin working with Terraform. Try running \"terraform plan\" to seeany changes that are required for your infrastructure. All Terraform commandsshould now work.If you ever set or change modules or backend configuration for Terraform,rerun this command to reinitialize your working directory. If you forget, othercommands will detect it and remind you to do so if necessary.Create a Key PairFor this project, we will be standing up an EC2 instance. AWS requires us to create key pairs so that we can connect to our EC2 instance. To do this, start by heading over to the EC2 service in the AWS console. Next click on Key Pairs on the left hand panel and then Create a Key Pair in the top right hand corner.Next up we need to give our key pair a name and choose a file format. I would personally choose the pem format because it already works with both Mac and Linux, and converting it to a ppk format later is easy if you want to use it with Windows. Once you click “Create Key Pair” it will automatically download for you.Create VPCFrom here on the theme of terraform will be to define AWS resources and specifying the details on what we want that resource to be once created. The cidr_block is going to be the network we are going to define now. Later on we will be making subnets within this network CIDR (Classless Internet Domain Routing). The naming being used in these resources could be anything, but for these examples I will be using “prod” or “production”.resource \"aws_vpc\" \"prod-vpc\" { cidr_block = \"10.0.0.0/16\" instance_tenancy = \"default\" tags = { Name = \"production\" }}Create Internet GatewayNow that we have already added a resouce to this terraform file, we are going to start seeing another theme surface which will be attaching new resources referrencing prior resources. The beggining of that theme starts here when we define our internet gateway. We want this gateway to be attached to the VPC we just made, so we use the following format of aws_vpc.prod-vpc.id.resource \"aws_internet_gateway\" \"gw\" { vpc_id = aws_vpc.prod-vpc.id}Create a SubnetSo from the network of 10.0.0.0/16 we created in the original VPC definition, we will create a subnet within that network. The network we will create for this example will be 10.0.1.0/24. We attach it to the VPC using the aws vpc id again.resource \"aws_subnet\" \"subnet-1\" { vpc_id = aws_vpc.prod-vpc.id cidr_block = \"10.0.1.0/24\" availability_zone = \"us-east-1a\" tags = { Name = \"prod-subnet\" }}Create Custom Route TableOnce again we are going to give this resource a name. We are going to attach the route table to our VPC with the same syntax as we used for the internet gateway. Finally, we label the tag. What we are creating here is a default gate in the route table. To set the default gateway, we use the “quad zero” subnet by using 0.0.0.0/0. This default route will send all IPv4 traffic to our to our gateway. The same thing is configured for IPv6, but instead of using the “quad zero,” we will represent this in the IPv6 equivalent which is ::/0. Again, this will route all IPv6 traffic to our gateway. At this point you would think this is where we would attach our subnet and route table together, but that is not the case. Rather, we will use another AWS resource called “Route Table Association” in the next section.resource \"aws_route_table\" \"prod-route-table\" { vpc_id = aws_vpc.prod-vpc.id route { cidr_block = \"0.0.0.0/0\" gateway_id = aws_internet_gateway.gw.id } route { ipv6_cidr_block = \"::/0\" gateway_id = aws_internet_gateway.gw.id } tags = { Name = \"production\" }}Associate Subnet with Route TableThis resource is fairly self explanatory. We are going to use this to associate our subnet with our route table.resource \"aws_route_table_association\" \"a\" { subnet_id = aws_subnet.subnet-1.id route_table_id = aws_route_table.prod-route-table.id}Create Security Group to Allow Port Web Traffic and SSH Warning: In this example project I am creating a web server that I want public facing and accessible for web traffic. I am also adding in SSH for connecting to the machine with remote access. For opening up ports in your AWS infrastructure, always have a reason and be mindful of the security risks you are exposing yourself to. This is just example.Security group ports are nearly the equivalent to the opening of ports on your firewall at home and doing port forwarding. Here we specify what ports that we want to allow traffic in from and out to our infrastructure. In this project the end result will be a web server, therefore we want to allow HTTPS (port 443) and HTTP (port 80) which is default web traffic. As we have done before, we are naming our resources, tagging them to be easily identifable when we are viewing from the AWS console, and attaching it to our VPC resource earlier defined. Since we are configuring incoming connections, we could limit the connections to only specific IPs or networks/subnets with the cidr_blocks and ipv6_cidr_blocks. Instead of limiting these connections, we want an entirely public facing result; therefor we will once again use the “quad zero” 0.0.0.0/0 and IPv6 equivalent ::/0.resource \"aws_security_group\" \"allow_web\" { name = \"allow_web_traffic\" description = \"Allow Web inbound traffic\" vpc_id = aws_vpc.prod-vpc.id ingress { description = \"HTTP\" from_port = 80 to_port = 80 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] ipv6_cidr_blocks = [\"::/0\"] }ingress { description = \"HTTPS\" from_port = 443 to_port = 443 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] ipv6_cidr_blocks = [\"::/0\"] }ingress { description = \"SSH\" from_port = 22 to_port = 22 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] ipv6_cidr_blocks = [\"::/0\"] } egress { from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [\"0.0.0.0/0\"] ipv6_cidr_blocks = [\"::/0\"] } tags = { Name = \"allow_web_also_ssh\" }}Create a Network Interface with an IP in the SubnetSo we have a lot of the networking set up at this point. We want to setup an IP that we are going to be using in this subnet with a NIC that we can later attach to an EC2 instance for our webserver. As always he call in the AWS resource we will use, network interface in this case, and give it a name. Next we attach the nic to prior resouces we defined such as the subnet and security group. Additionally, we are going to go with the IP address of 10.0.1.50.resource \"aws_network_interface\" \"web-server-nic\" { subnet_id = aws_subnet.subnet-1.id private_ips = [\"10.0.1.50\"] security_groups = [aws_security_group.allow_web.id]}Assign an Elastic IP to the Network InterfaceIf we were to spin up our EC2 instance right now as is, we would get an IP address for our subnet and networking would work. However, we want a little more control of this aspect in this project. To achieve the control we are looking for, we will set an Elastic IP. An Elastic IP address is a static public IPv4 address associated with your AWS account in a specific Region. Unlike an auto-assigned public IP address, an Elastic IP address is preserved after you stop and start your instance in a virtual private cloud (VPC). Tip: The AWS Elastic IP relies on the deployment of the internet gateway. If you try to create this EIP in a VPC with no internet gateway then it will throw and error. The solution to this when using terraform is to specify the depends_on and specifying the aws_internet_gateway.&lt;YOUR-RESOURCE-NAME&gt;.resource \"aws_eip\" \"one\" { vpc = true network_interface = aws_network_interface.web-server-nic.id associate_with_private_ip = \"10.0.1.50\" depends_on = [ aws_internet_gateway.gw ]}Create Debian Server then Install and Enable Apache2Finally we have our networking the way we want it. The fun part of creating our actual webserver is here! A big piece of this part is specifing the AMI (Amazon Machine Image). We want to use a Debian instance to house our web server and this can be done by finding the correct AMI ID. It’s important to check that the AMI ID you are choosing is correct because the AMI IDs supplied by Amazon are subject to change if you will be using theirs. To find the most up to date ID, open up the AWS console, then navigate to the EC2 service, click launch an instance from the top right hand corner of the screen, type “debian” in the search bar and press the enter key.The results of AMIs supplied by amazon should appear with Amazon’s latest Debian version. Here is where you will see two AMI IDs. One is for the x86 architecture while the other is an ARM architecture. We will be using the x86 for this project. On top of choosing the correct architecture, as always doing anything in the AWS console, make sure that you are doing all of these actions in the intended region you plan to deploy your infrastructure in. You can search for Debian in the Amazon catalog and the “correct” AMI will be the result. However, the AMI IDs are different for the same “Debian” across different regions. Note: Choose the correct AMI ID!Since this project is simply for going through the basics of terraform and we are only using a basic web server as an example, we do not need a lot of performance and power here. The free-tier instance type of t2.micro will be perfect for this situation. We will attach the key pair we made at the beggining of this post so that we can use the .pem file we downloaded earlier to now access the EC2 instance using SSH and remotely through the port 22 we opened earlier in the security group. We will also attach the NIC we created and configured earlier with the Elasitc IP. Last but not least, we are going to have terraform perform some commands automatically on startup of our EC2 instance by having it update our package manager aptitude, install apache2 and auto approve the prompt, start the daemon using systemd, and then overwrite our starting index.html file with our super basic “This is my webserver” text.resource \"aws_instance\" \"web-server-instance\" { ami = \"ami-07d02ee1eeb0c996c\" instance_type = \"t2.micro\" availability_zone = \"us-east-1a\" key_name = \"terraform-aws-key\" network_interface { device_index = 0 network_interface_id = aws_network_interface.web-server-nic.id } user_data = &lt;&lt;-EOF #!/bin/bash sudo apt update -y sudo apt install apache2 -y sudo systemctl start apache2 sudo bash -c 'echo This is my webserver &gt; /var/www/html/index.html' EOF tags = { Name = \"web-server-instance\" }}Terraform PlanAll the pieces are in place. Now it’s time to start watching the magic happen with the following command:terraform planThis will go through a dry run of what changes will be implemented if the current start of our terraform file will be deployed based on what the existing state of current infrastructure is. At the moment, since we have not deployed anything yet, we will see the following output:Plan: 9 to add, 0 to change, 0 to destroy.Terraform ApplyEverything is looking good! I can review than pending changes and send it on it’s way when I am satisfied with the following command:terraform applyYou will once again be prompted of the changes and a detailed list of those changes to once again review. A manual user input is required to continue by entering “yes”.Plan: 9 to add, 0 to change, 0 to destroy.Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value:The following output will result in a successful deploy of our AWS infrastructure defined in our terraform.aws_vpc.prod-vpc: Creating...aws_vpc.prod-vpc: Creation complete after 2s [id=vpc-01477f886e172762b]aws_internet_gateway.gw: Creating...aws_subnet.subnet-1: Creating...aws_security_group.allow_web: Creating...aws_internet_gateway.gw: Creation complete after 1s [id=igw-0e4d81991a308a8a2]aws_route_table.prod-route-table: Creating...aws_subnet.subnet-1: Creation complete after 1s [id=subnet-0480fd0a6ab8e5593]aws_route_table.prod-route-table: Creation complete after 2s [id=rtb-0b7d8e33ef8f17370]aws_route_table_association.a: Creating...aws_route_table_association.a: Creation complete after 1s [id=rtbassoc-0969419db4a5db64a]aws_security_group.allow_web: Creation complete after 4s [id=sg-041a669fc2ae109b4]aws_network_interface.web-server-nic: Creating...aws_network_interface.web-server-nic: Creation complete after 1s [id=eni-07a1016a60e537708]aws_eip.one: Creating...aws_instance.web-server-instance: Creating...aws_eip.one: Creation complete after 2s [id=eipalloc-09f476c9598eb40bb]aws_instance.web-server-instance: Still creating... [10s elapsed]aws_instance.web-server-instance: Still creating... [20s elapsed]aws_instance.web-server-instance: Creation complete after 24s [id=i-037bdb9db1dcedfad]Apply complete! Resources: 9 added, 0 changed, 0 destroyed.Checking the ResultsNow our terraform is setup to configure our AWS innfrastructure exactly how we want it. It will take only a few moments to configure and it will all be deployed automatically. Best of all our environment will be exactly the same each time without errors (unless of course if you made any mistakes in the .tf file itself).We can view the results on the AWS Console. Info: This EC2 instance and the AWS Account are only temporary. This information will not be used by me in the time of publishing.We can test the result by visiting the public IP Elastic IP that was assigned.Terraform DestoryOf course what goes up, must come down. Well, not “must”; but if you need to tear down everything that was just deployed using terraform, we can use the following command to spin down the infrastructure.terraform destroyYou will a detailed list of all the resources that will destroyed with this command. If everything looks as desired, a manual entry of the keyword “yes” will be required.Plan: 0 to add, 0 to change, 9 to destroy.Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only 'yes' will be accepted to confirm. Enter a value:After tying “yes” and pressing enter, you will see the following output that your resources were successfully destroyed.aws_route_table_association.a: Destroying... [id=rtbassoc-0969419db4a5db64a]aws_eip.one: Destroying... [id=eipalloc-09f476c9598eb40bb]aws_instance.web-server-instance: Destroying... [id=i-037bdb9db1dcedfad]aws_route_table_association.a: Destruction complete after 0saws_route_table.prod-route-table: Destroying... [id=rtb-0b7d8e33ef8f17370]aws_route_table.prod-route-table: Destruction complete after 1saws_eip.one: Destruction complete after 2saws_internet_gateway.gw: Destroying... [id=igw-0e4d81991a308a8a2]aws_internet_gateway.gw: Destruction complete after 1saws_instance.web-server-instance: Still destroying... [id=i-037bdb9db1dcedfad, 10s elapsed]aws_instance.web-server-instance: Still destroying... [id=i-037bdb9db1dcedfad, 20s elapsed]aws_instance.web-server-instance: Still destroying... [id=i-037bdb9db1dcedfad, 30s elapsed]aws_instance.web-server-instance: Destruction complete after 31saws_network_interface.web-server-nic: Destroying... [id=eni-07a1016a60e537708]aws_network_interface.web-server-nic: Destruction complete after 0saws_subnet.subnet-1: Destroying... [id=subnet-0480fd0a6ab8e5593]aws_security_group.allow_web: Destroying... [id=sg-041a669fc2ae109b4]aws_subnet.subnet-1: Destruction complete after 1saws_security_group.allow_web: Destruction complete after 1saws_vpc.prod-vpc: Destroying... [id=vpc-01477f886e172762b]aws_vpc.prod-vpc: Destruction complete after 1sDestroy complete! Resources: 9 destroyed.Completed main.tf FileAt the end of all this, your main.tf file will come together to look like the following:main.tfprovider \"aws\" { region = \"us-east-1\" access_key = \"&lt;YOUR-ACCESS-KEY&gt;\" secret_key = \"&lt;YOUR-SECRET-KEY&gt;\"}resource \"aws_vpc\" \"prod-vpc\" { cidr_block = \"10.0.0.0/16\" instance_tenancy = \"default\" tags = { Name = \"production\" }}resource \"aws_internet_gateway\" \"gw\" { vpc_id = aws_vpc.prod-vpc.id}resource \"aws_subnet\" \"subnet-1\" { vpc_id = aws_vpc.prod-vpc.id cidr_block = \"10.0.1.0/24\" availability_zone = \"us-east-1a\" tags = { Name = \"prod-subnet\" }}resource \"aws_route_table\" \"prod-route-table\" { vpc_id = aws_vpc.prod-vpc.id route { cidr_block = \"0.0.0.0/0\" gateway_id = aws_internet_gateway.gw.id } route { ipv6_cidr_block = \"::/0\" gateway_id = aws_internet_gateway.gw.id } tags = { Name = \"production\" }}resource \"aws_route_table_association\" \"a\" { subnet_id = aws_subnet.subnet-1.id route_table_id = aws_route_table.prod-route-table.id}resource \"aws_security_group\" \"allow_web\" { name = \"allow_web_traffic\" description = \"Allow Web inbound traffic\" vpc_id = aws_vpc.prod-vpc.id ingress { description = \"HTTP\" from_port = 80 to_port = 80 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] ipv6_cidr_blocks = [\"::/0\"] }ingress { description = \"HTTPS\" from_port = 443 to_port = 443 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] ipv6_cidr_blocks = [\"::/0\"] }ingress { description = \"SSH\" from_port = 22 to_port = 22 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] ipv6_cidr_blocks = [\"::/0\"] } egress { from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [\"0.0.0.0/0\"] ipv6_cidr_blocks = [\"::/0\"] } tags = { Name = \"allow_web_also_ssh\" }}resource \"aws_network_interface\" \"web-server-nic\" { subnet_id = aws_subnet.subnet-1.id private_ips = [\"10.0.1.50\"] security_groups = [aws_security_group.allow_web.id]}resource \"aws_eip\" \"one\" { vpc = true network_interface = aws_network_interface.web-server-nic.id associate_with_private_ip = \"10.0.1.50\" depends_on = [ aws_internet_gateway.gw ]}resource \"aws_instance\" \"web-server-instance\" { ami = \"ami-07d02ee1eeb0c996c\" instance_type = \"t2.micro\" availability_zone = \"us-east-1a\" key_name = \"terraform-aws-key\" network_interface { device_index = 0 network_interface_id = aws_network_interface.web-server-nic.id } user_data = &lt;&lt;-EOF #!/bin/bash sudo apt update -y sudo apt install apache2 -y sudo systemctl start apache2 sudo bash -c 'echo This is my webserver &gt; /var/www/html/index.html' EOF tags = { Name = \"web-server-instance\" }}" }, { "title": "Checking AWS S3 Buckets With Python Lambda Function", "url": "/posts/CheckS3BucketsWithLambdaFunction/", "categories": "Cloud Providers, Amazon Web Services (AWS)", "tags": "s3 buckets, python, aws, automation, lambda", "date": "2022-06-27 10:00:00 -0700", "snippet": "This will be a basic example on using a python Lambda function to check S3 Buckets.Table of Contents Create a Lambda Function in AWS Python Code Source for Function Explaining the Code Source Test Event Configure Troubleshoot and Resolve Test Create a Lambda Function in AWSOnce you are signed into your AWS Account, navigate to the Lambda &gt; Functions and create a functionIn the creation screen, we don’t really need to change much other than giving our function a name. In this case I am creating a Lambda function to check the S3 buckets so I am going with “S3BucketCheck”Python Code Source for FunctionNow that we have the Lambda function created in our AWS account, it’s time to configure it to perform what actions we want it take with our python code.import jsonimport boto3s3 = boto3.resource('s3')def lambda_handler(event, context): bucket_list = [] for bucket in s3.buckets.all(): print(bucket.name) bucket_list.append(bucket.name) return { 'statusCode': 200, 'body': bucket_list }Explaining the Code SourceThe Amazon python SDK is called boto3 and readily available for you to use in the Lambda development environment so there is no need to install anything. Simply “import boto3”Next we setup the service object that we are going to be interfacing with. This is done by calling boto3 and then specifying resource and then calling what resource we want to interact with. ('s3') in this caseNext we set up the Lambda Function itself by starting with a function using def. We will use lambda_handler as a convention and pass an event and context.In the function, we will create an emtpy list named bucket_list that will later contain the names of the s3 buckets this function will go fetch for us.We are going to use a for loop to iterate through the s3 bucket object for all buckets and then print those names using bucket.name as we add them to our empty list using append.Finally we can create a return status from the results of the function. Here we want a status code and to give us the list it created from what was found in our S3 Buckets list of our AWS account.Deploy your changes!Test EventWe have created a function in AWS, and filled in our code source using python. Now moving onto the test event.ConfigureTo configure, we will first give our test event a name. For this example I am using “S3List”. We can leave the Event JSON as default key values and click “Save”.Troubleshoot and ResolveIf we run our test as it is you will an error occured (AccessDenied) when calling the ListBuckets operation: Access Denied.To resolve this we will need to give our lambda function’s role in AWS the proper permissions to interact with the S3 buckets.Head over to IAM &gt; Roles and you will see your auto generated lambda role from when you first created the lambda function in AWS. Warning: I’m going to be giving my lambda function full S3 access for the sake of this example. There are best practices out there that should be followed for proper permissions for something such as this. Please read AWS Lambda Permissions hereOpen the lambda role and “Add Permissions” then “Attach Policies”Filter for “S3” and check the box to choose the AmazonS3FullAccess policy and then click Attach Policies.Now we have two policies on our lambda function role and we can check that we have added the correct permissions here.As we can see, if we re-run our test will are getting the list of names for the S3 Buckets that I created earlier for this test.TestTest Event NameS3ListResponse{ \"statusCode\": 200, \"body\": [ \"hereismybucket01\", \"hereismybucket02\", \"hereismybucket03\", \"hereismybucket04\", \"hereismybucket05\" ]}Function LogsSTART RequestId: 70c76265-bf1f-464a-a50e-7ae62a0408d9 Version: $LATESThereismybucket01hereismybucket02hereismybucket03hereismybucket04hereismybucket05END RequestId: 70c76265-bf1f-464a-a50e-7ae62a0408d9REPORT RequestId: 70c76265-bf1f-464a-a50e-7ae62a0408d9\tDuration: 217.31 ms\tBilled Duration: 218 ms\tMemory Size: 128 MB\tMax Memory Used: 75 MBRequest ID70c76265-bf1f-464a-a50e-7ae62a0408d9And with this, we can see we were able to successfully pull a list of our S3 Buckets using a Python Lambda function.Future ThoughtsWhile this is a simply a dip of the toe in the waters of AWS Lambda, I have ideas to dig deeper into this topic by something along the lines of using ACM to send events via Cloudwatch and using a lambda function to handle the renewal. For example, add the magic DNS entry to the DNS of choice (if Route 53 is not being used) and then retrieve the cert and scp it to the ec2 instance." }, { "title": "Provision a Linux Server With Docker Using Ansible", "url": "/posts/ProvisionDockerWithAnsible/", "categories": "Infrastructure as Code, Ansible", "tags": "ansible, linux, docker, automation, docker-compose", "date": "2022-06-12 10:00:00 -0700", "snippet": "Using Ansible to provision remote servers with Docker and Docker-compose.Table of Contents Prerequisites Structure Files provisioning/hosts.yml provisioning/site.yml provisioning/host-vars/remote provisioning/roles/setup/handlers/main.yml provisioning/roles/setup/tasks/docker.yml provisioning/roles/setup/tasks/main.yml Build TestPrerequisitesFirst of all, local server should be able to access to remote server over SSH so let’s do it.Generating a SHH key# Current status on localansible@local:~$ ls -l ~/.ssh/-rw------- 1 ansible ansible 389 Mar 24 09:17 authorized_keys # Generate SSH keypair on localansible@local:~$ ssh-keygen -t rsa # New status on localansible@local:~$ ls -l ~/.ssh/-rw------- 1 ansible ansible 389 Mar 24 09:17 authorized_keys-rw------- 1 ansible ansible 1679 Mar 24 09:29 id_rsa-rw-r--r-- 1 ansible ansible 394 Mar 24 09:29 id_rsa.pubTransferring public key# Current status on remoteubuntu@remote:~$ ls -l ~/.ssh/-rw------- 1 ubuntu ubuntu 389 Mar 24 09:02 authorized_keys # Current authorised users on remoteubuntu@remote:~$ cat ~/.ssh/authorized_keysssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC16 vagrant # Copy public SSH key from localansible@local:~$ cat ~/.ssh/id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDDUkN ansible@local # Transfer local public SSH key over remoteubuntu@remote:~$ echo 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDDUkN ansible@local' &gt;&gt; ~/.ssh/authorized_keys # Current authorised users on remoteubuntu@remote:~$ cat ~/.ssh/authorized_keysssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC16 vagrantssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDDUkN ansible@localTesting SSH connectionansible@local:~$ ssh ubuntu@192.168.99.30Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-92-generic x86_64) Last login: Sat Mar 24 09:09:36 2018 from 10.0.2.2ubuntu@remote:~$Install Ansible on local serveransible@local:~$ sudo apt-add-repository ppa:ansible/ansibleansible@local:~$ sudo apt-get updateansible@local:~$ sudo apt-get install ansible ansible@local:~$ ansible --versionansible 2.5.0 config file = /etc/ansible/ansible.cfg configured module search path = [u'/home/ubuntu/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules'] ansible python module location = /usr/lib/python2.7/dist-packages/ansible executable location = /usr/bin/ansible python version = 2.7.12 (default, Dec 4 2017, 14:50:18) [GCC 5.4.0 20160609]StructureThis is the file and folder structure we are aiming for after creating all of our yaml files.$ tree.└── provisioning ├── host_vars │ └── remote ├── hosts.yml ├── roles │ └── setup │ ├── handlers │ │ └── main.yml │ └── tasks │ ├── docker.yml │ └── main.yml └── site.ymlFilesprovisioning/hosts.ymlall: hosts: remote: ansible_connection: ssh ansible_user: ubuntu # Remote user ansible_host: 192.168.99.30 # Remote host ansible_port: 22provisioning/site.yml---# This playbook sets up whole stack. - name: Configurations to \"remote\" host hosts: remote remote_user: ubuntu # Remote user become: yes roles: - setupprovisioning/host-vars/remote---# Variables listed here are applicable to \"setup\" role ansible_python_interpreter: /usr/bin/python3 remote_user: ubuntudocker_group: dockerprovisioning/roles/setup/handlers/main.yml---# This playbook contains common handlers that can be called in \"setup\" tasks. # sudo systemctl enable docker- name: Start docker on boot systemd: name: docker state: started enabled: yesprovisioning/roles/setup/tasks/docker.yml---# This playbook contains docker actions that will be run on \"remote\" host. # sudo apt-get install *- name: Install docker packages apt: name: \"\" state: present update_cache: yes with_items: - apt-transport-https - ca-certificates - curl - software-properties-common tags: - docker # curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -- name: Add Docker s official GPG key apt_key: url: https://download.docker.com/linux/ubuntu/gpg state: present tags: - docker # sudo apt-key fingerprint 0EBFCD88- name: Verify that we have the key with the fingerprint apt_key: id: 0EBFCD88 state: present tags: - docker # sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable\"- name: Set up the stable repository apt_repository: repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable state: present update_cache: yes tags: - docker # sudo apt-get update- name: Update apt packages apt: update_cache: yes tags: - docker # sudo apt-get install docker-ce=18.03.*- name: Install docker apt: name: docker-ce=18.03.* state: present update_cache: yes notify: Start docker on boot tags: - docker # sudo groupadd docker- name: Create \"docker\" group group: name: \"\" state: present tags: - docker # sudo usermod -aG docker ubuntu- name: Add remote \"ubuntu\" user to \"docker\" group user: name: \"\" group: \"\" append: yes tags: - docker # sudo apt-get install docker-compose=1.8.*- name: Install docker-compose apt: name: docker-compose=1.8.* state: present update_cache: yes tags: - dockerprovisioning/roles/setup/tasks/main.yml---# This playbook contains all actions that will be run on \"local\" host. # sudo apt-get update- name: Update apt packages apt: update_cache: yes tags: - system # Import docker tasks- name: Import docker tasks include_tasks: docker.yml # sudo apt-get autoclean- name: Remove useless apt packages from the cache apt: autoclean: yes tags: - system # sudo apt-get autoremove- name: Remove dependencies that are no longer required apt: autoremove: yes tags: - systemBuild$ ansible-playbook provisioning/site.yml -i provisioning/hosts.yml PLAY [Configurations to \"remote\" host] ************************************************* TASK [Gathering Facts] *****************************************************************ok: [remote] TASK [setup : Update apt packages] *****************************************************changed: [remote] TASK [setup : Import docker tasks] *****************************************************included: /var/www/html/application/provisioning/roles/setup/tasks/docker.yml for remote TASK [setup : Install docker packages] *************************************************ok: [remote] =&gt; (item=[u'apt-transport-https', u'ca-certificates', u'curl', u'software-properties-common']) TASK [setup : Add Docker's official GPG key] *******************************************changed: [remote] TASK [setup : Verify that we have the key with the fingerprint] ************************ok: [remote] TASK [setup : Set up the stable repository] ********************************************changed: [remote] TASK [setup : Update apt packages] *****************************************************changed: [remote] TASK [setup : Install docker] **********************************************************changed: [remote] TASK [setup : Create \"docker\" group] ***************************************************ok: [remote] TASK [setup : Add remote \"ubuntu\" user to \"docker\" group] ******************************changed: [remote] TASK [setup : Remove useless apt packages from the cache] ******************************ok: [remote] TASK [setup : Remove dependencies that are no longer required] *************************ok: [remote] TASK [setup : Install docker-compose] **************************************************changed: [remote] RUNNING HANDLER [setup : Start docker on boot] *****************************************ok: [remote] PLAY RECAP *****************************************************************************remote : ok=14 changed=7 unreachable=0 failed=0Test# SSH into remote serveransible@local:~$ ssh ubuntu@192.168.99.30 # Docker compose versionubuntu@remote:~$ docker-compose --versiondocker-compose version 1.8.0, build unknown # Docker versionubuntu@remote:~$ docker -vDocker version 18.03.0-ce, build 0520e24 # Check if \"ubuntu\" user member of \"docker\" groupubuntu@remote:~$ groupsdocker adm dialout cdrom floppy sudo audio dip video plugdev netdev lxd # Check if \"ubuntu\" user can run \"docker\" commandsubuntu@remote:~$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES" }, { "title": "Portainer Documented", "url": "/posts/PortainerDocumented/", "categories": "Containers, Docker", "tags": "docker, containers, webui, management", "date": "2022-06-07 10:00:00 -0700", "snippet": "Portainer is a simple and lightweight, but powerful application that is used to provide a web management interface that you can use to perform functions on your Docker host.Table of Contents Uisng Portainer to Manage Docker How to view Container Status How to run a container Name and Image Ports Advanced Container Settings How to use the Application Templates in Portainer How to Start and Stop Containers How to Create and Manage Volumes How to Pull and Manage Docker Container Images How to create and manage networks How to attach networks to running containers How to view container logs How to access the console within a container How to view the container statistics How to create a new container imageUisng Portainer to Manage DockerOnce you login to Portainer you will be presented with the home screen. If you set-up Portainer properly, then you should see your local docker server on the screen. You can click that server to view your dashboard.The main navigation menu is in the left sidebar. From the sidebar, you can view your dashboard, running containers, volumes, networks, images, and moreHow to view Container StatusTo view the status of your running containers, click the Containers link on the left sidebar. If you have containers running, your screen will look similar to the image below.At a glance you can see important information such as name of the container, state of the container, the image that the container is using, the the containers internal IP address, and the exposed container ports that you can access.From this screen, you will be able to perform most of the basic functions to control your containers.How to run a containerIn order to run a container, you can click the button labeled + Add container. Once you click the button you will be directed to the “Create a container” page.Name and ImageThis is the page that you will begin entering the details of the container you would like to run. The name field is equivalent to –name flag in a typical Docker run command. You can use the image field to type in the name of the image from the Docker Hub that you would like to run. Selecting “Always pull the image” will ensure that you pull the latest image from the hub rather than using an older, out of date image that may be on your system.PortsNext you can choose if you want to publish all network ports that are exposed by the container to random host ports. This will randomize the ports that the host system uses to map to the container every time the container restarts. If you enable this option and the points port 80 in the container to port 7840 then you will have to access your container by visiting http://dockerhostip:7840. Keep in mind that if the container is recreated after it is stopped you will be given a different port number that replaces port 7840 causing you to lose access to your application at the previous port.Manually mapping the ports to the container will ensure that the ports mappings remain persistent if you would need to stop and restart the container. You can map your ports manually by clicking the “publish a new network port” button on the create a container screen. Once the button is clicked two fields will pop up for you to fill out.Type the port number that you would like exposed from the host system in the “host” field and in the “container” field, type the port number that the host container needs to be mapped to.For example, if you are running a container that contains a web application that needs to be accessed on port 80. You would type 80 in the container field. You can choose any unused port on the host machine to map to the container port. Let’s use port 8080. Now, once the container is up and running, we can access the web application at http://dockerhostip:8080.Using this function in Portainer is equivalent to the -p flag in the Docker run command.If you don’t have any advanced settings to configure such as adding persistent volumes, changing the networks or adding environmental variables then from here you can click “Deploy Container” to run your container.Advanced Container SettingsIf you continue to scroll down on the “Create a container” page you will see the “Advanced Container Settings” section.Below is a table that outlines the functions in the “Advanced Containers Settings” section. Tab Description Command &amp; Logging Allows you to specify a startup command, entry point, working directory, and user on container startup. You can also specify if you need the container to run in interactive, detached, TTY, or multiple modes. Volumes Allows you to map volumes to your container (the -v flag within the Docker Run command). It also allows you to bind directories on the host machine to directories within the container. In addition, you can make a volume read-only. Network From here you can determine which available network on the host system that you want your container to be connected to. You can also add a hostname, domain name, mac address, IPv4 or IPv6 Address, and a Primary and secondary DNS Server. You can also add entries to the containers hosts file from this tab. Env Within the “Env” tab you can add environmental variable to your container. When you click the “add environmental variable” you will be asked to enter your variable name and value. This is equivalent to the -e flag within the Docker Run command. Labels This tab allows you to add labels to your container. When you click the “add label” you will be asked to enter your label name and value. This is equivalent to the -l flag within the Docker Run command. Restart Policy From here you can determine your container restart policy. You can select none, always, on failure, and unless stopped. Runtime &amp; Resources This tab will allow you to configure the containers runtime &amp; resource information. From here you can select to run the container in privileged mode, configure the runtime, attach devices from the host, and lime the amount of resources on the host that the container is allowed to utilize. Capabilities From this tab you can select a multitude of capability options to add to your container. The available options include, but aren’t limited to net_admin, net_broadcast, audit_write, and audit_control. How to use the Application Templates in PortainerIf you don’t want the hassle of running a container from scratch then you’re in luck! Portainer has a preset list of application templates to that pre-fills out the “Create a container” form for you. Just click on “Application Templates” in the left navigation sidebar, click the application you want to run, fill in the name field, and click deploy.A few seconds later, your new application will be up and running and ready to access.How to Start and Stop ContainersTo issue basic start and stop commands to your containers, click on “Containers” in the left sidebar of Portainer. Next, tick the box to the left of the container and then select the function you want to perform from the function bar (as seen above). From this bar you can start, stop, kill, restart, pause, resume, and delete any of the containers on your docker host.You can also click on the name of the container. Once you are on the container details page, you can select the function that you want to perform on the top bar as well.How to Create and Manage VolumesTo manage your docker volumes click on “Volumes” in the left navigation sidebar. From here you can see all of the volumes on your Docker host. If you would like to view the details of a volume, just click the name of the volume. Once you open up the volume you will see a screen similar to the screenshot below.From here you can see the name of the volume and delete the volume from your system if you need to. In addition, if you are using the NFS driver you will see an additional section that looks like the screenshot below.From here you can see the NFS link and options string that is being used. You cannot make changes to volumes in Portainer once they are created, you can only delete them.How to Pull and Manage Docker Container ImagesYou are able to download container images from Docker Hub into your docker server by simply clicking on the “Images” link on the left navigation sidebar. You will see a screen like the one in the screenshot below.You can pull the image by typing the image tag in the “image” text field. It should be in the format of username/image_name. Once you type in your docker image tag, click “Pull the image”. This will download the image to your docker host.In addition to downloading images to your docker host, you are also able to manage the images that are already on the host. Once you have a few containers running, you will see them in a list like the one below.From this list you can click on the ID of an image to learn more information about the image. You can also tick the box to the left of the image name and click remove image.Make sure to check your image section often. When you delete containers from your system, the images still stay stored on the Docker host. This can quickly take up a lot of disk space if you run and experiment with a lot of different containers.How to create and manage networksIn order to manage your networks in Docker, click on the “Networks” link on the left navigation sidebar. You should then see a screen that looks similar to the screenshot below.If you need to delete a network from your system, just tick the box to the left of the network name and click the remove button.In order create a new network on your Docker host click “Add network”. You’ll then be taken to a screen that will allow you to quickly add your network.In the name field, type out a name for your new network. Next, you can select a driver. You can choose between bridge, host, ipvlan, macvlan, null, and overlay. More information about the network drivers here.After you select your driver, you can begin adding your network information such as the Subnet, IP range and Gateway. Under the advanced configuration section, you can configure labels, determine if you would like to restrict external access to the network, and enable manual container attachment.How to attach networks to running containersIf you have manual container attachment enabled on one of your networks. You can go back to the container list by clicking the container link in the left navigation sidebar. After you see yours list of containers you can click the name of one to view it’s details. When you scroll down to the bottom of the page, you will see a section called “Connected networks” that looks like the screenshot below.To attach one of your newly created networks from the container, click the “Join a network” drop down list, select your network, and click the “Join network” button.How to view container logsPortainer also allows you to view container logs so you can diagnose problems and ensure that your container is running correctly.In order to view the logs, click the paper icon in the quick actions column as indicated above. Once you click the button you will be directed to the log viewer page. From here you can view the logs and adjust the log viewer settings.You can also get to the log viewer page by clicking the “Logs” icon as seen below within the Container Details page.How to access the console within a containerIf you would like to access the console within a container, it can be done with Portainer as well. This time click the “&gt;_” Icon within the Quick actions bar on the Container dashboard as seen below.Once you click the icon, you can click the “connect” button on the next screen. You’ll then be taken to the terminal window.As you can see I ran the pihole -c command on my instance of Pi-Hole. Having access to the console is super helpful if you are away from your network and have Portainer running behind a reverse proxy.You can also get to the log viewer page by clicking the “Console” icon as seen below within the Container Details page.How to view the container statisticsYou now have your container up and running in Portainer, but how can you monitor it’s resource utilization on the host? Simply click on the graph icon within the quick actions column in your running container within the container dashboard. As shown above.From this screen you will be able to view your the memory, CPU and network usage of your container. You can easily change the automatic update duration by selecting how many seconds you want the stats to update at the top of the screen.If you scroll down, you will see is view a list of running processes. This allows you to diagnose what command or process is using up the resources on your docker host from within the container.You can also get to the log viewer page by clicking the “Stats” icon as seen below within the Container Details page.How to create a new container imageIf you make modifications to a running container within Portainer, you can easily update the image on your local system. All you have to do is go to the container details of any container within the container dashboard and scroll to the “Create image” section. This will allow you to quickly create and save an image to the docker host from the running image.You can create an image by typing in whatever image tag that you come up with and clicking the “Create” button.After docker saves the new image, you can view it in the image management dashboard. If you need to run it, just type the tag name on the “Create a container” screen within the container management dashboard.You can then view and manage your newly created container in the Image Management Dashboard by clicking “Images” on the left navigation sidebar." }, { "title": "Deploying VMs With Terraform In ProxMox", "url": "/posts/DeployingVMsWithTerraformInProxMox/", "categories": "Infrastructure as Code, Terraform", "tags": "terraform, proxmox, cloudinit, automation", "date": "2022-06-05 10:00:00 -0700", "snippet": "Leveraging our ProxMox cloud-init templates to deploy VMs entirely through Terraform quickly and easily.Table of Contents Prep a cloud-init template to use with Terraform Install qemu-guest-agent on cloud-init template Install Terraform Creating authentication for Terraform to make changes to ProxMox via API keys Terraform basic initialization and provider installation Develop Terraform plan main.tf vars.tf Terraform plan Run Terraform plan and watch the VMs appear! Terraform DestroyPrep a cloud-init template to use with TerraformSo I will very briefly recap the commands from a previous post that will get you a working cloud init template for ProxMox so that we can use it with Terraform to automate deploying VMs. Please check the previous post if you want more details. Download Debian Cloud Image wget https://cloud.debian.org/images/cloud/bullseye/20220613-1045/debian-11-genericcloud-amd64-20220613-1045.qcow2 Create VM and Convert to Template qm create 9500 --name Debian11CloudInit --net0 virtio,bridge=vmbr0qm importdisk 9500 debian-11-genericcloud-amd64-20220613-1045.qcow2 local-lvmqm set 9500 --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-9500-disk-0qm set 9500 --ide2 local-lvm:cloudinitqm set 9500 --boot c --bootdisk scsi0qm set 9500 --serial0 socket --vga serial0qm set 9500 --agent enabled=1qm template 9500 Install qemu-guest-agent on cloud-init templateNow there is an issue if you simply leave the cloud-init template with just the commands above. If you call on terraform apply to deploy the VMs using the image without the qemu-guest-agent installed it will forever be stuck “still creating...” as show in the image below.To resolve this we first need to clone the template we created previously as a VM we can boot and log into to install our qemu-guest-agentqm clone 9500 9000 --name KubernetesNodeTemplateStart the VMqm start 9000Log into the VM. I’ll be using an ssh key that I configured with my cloud-init for this template over the network, but you can also simply use the console from the ProxMox Web UI if you set up a password for the default user.ssh -i cloud-init brandon@192.168.2.191Make sure everything is up to date, install the qemu-guest-agent, then shutdown the VM.sudo apt updatesudo apt install qemu-guest-agentsudo shutdown nowWith these steps out of the way, we are ready to convert our VM to a template to be used by terraform.qm template 9000Install TerraformNow we need to install terraform to our control system if it is not already there.curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -sudo apt-add-repository \"deb [arch=$(dpkg --print-architecture)] https://apt.releases.hashicorp.com $(lsb_release -cs) main\"sudo apt updatesudo apt install terraformCreating authentication for Terraform to make changes to ProxMox via API keysThere are probably better settings for this section to more securely lock down this user but this will at least get you started with using terraform on ProxMox.So we need to create a new user. We’ll name it ‘terraform’. To add a new user go to Datacenter in the left tab, then Permissions -&gt; Users -&gt; Click add, name the user and click add.Next, we need to add API tokens. Click API tokens below users in the permissions category and click add. Select the user you just created and give the token an ID, and uncheck privilege separation (which means we want the token to have the same permissions as the user):When you click Add it will show you the key. Save this key. It will never be displayed again!Next we need to add a role to the new user. Permissions -&gt; Add -&gt; Path = ‘/’, User is the one you just made, role = ‘PVEVMAdmin’. This gives the user (and associated API token!) rights to all nodes (the / for path) to do VMAdmin activities:You also need to add permissions to the storage used by the VMs you want to deploy (both from and to), for me this is /storage/local-lvm. Use Admin for the role here because the user also needs the ability to allocate space in the datastore (you could use PVEVMAdmin + a datastore role):We are now done with the permissions. It is time for Terraform!Terraform basic initialization and provider installationTerraform has three main stages: init, plan, and apply. We will start with describing the plans, which can be thought of a a type of configuration file for what you want to do. Plans are files stored in directories. Make a new directory (terraform), and create two files: main.tf and vars.tf:cd ~mkdir terraform &amp;&amp; cd terraformtouch main.tf vars.tfThe two files are hopefully reasonably named. The main content will be in main.tf and we will put a few variables in vars.tf. Everything could go in main.tf but it is a good practice to start splitting things out early. I actually don’t have as much in vars.tf as I should but we all gotta start somewhereOk so in main.tf let’s add the bare minimum. We need to tell Terraform to use a provider, which is the term they use for the connector to the entity Terraform will be interacting with. Since we are using Proxmox, we need to use a Proxmox provider. This is actually super easy – we just need to specify the name and version and Terraform goes out and grabs it from github and installs it. I used the Telmate Proxmox provider.main.tf:terraform { required_providers { proxmox = { source = \"telmate/proxmox\" version = \"2.7.4\" } }}Save the file. Now we’ll initialize Terraform with our barebones plan (terraform init), which will force it to go out and grab the provider. If all goes well, we will be informed that the provider was installed and that Terraform has been initialized. Terraform is also really nice in that it tells you the next step towards the bottom of the output (“try running ‘terraform plan’ next”).brandon@ProxMox:~/terraform# terraform initInitializing the backend...Initializing provider plugins...- Finding telmate/proxmox versions matching \"2.7.4\"...- Installing telmate/proxmox v2.7.4...- Installed telmate/proxmox v2.7.4 (self-signed, key ID A9EBBE091B35AFCE)Partner and community providers are signed by their developers.If you'd like to know more about provider signing, you can read about it here:https://www.terraform.io/docs/cli/plugins/signing.htmlTerraform has created a lock file .terraform.lock.hcl to record the providerselections it made above. Include this file in your version control repositoryso that Terraform can guarantee to make the same selections by default whenyou run \"terraform init\" in the future.Terraform has been successfully initialized!You may now begin working with Terraform. Try running \"terraform plan\" to seeany changes that are required for your infrastructure. All Terraform commandsshould now work.If you ever set or change modules or backend configuration for Terraform,rerun this command to reinitialize your working directory. If you forget, othercommands will detect it and remind you to do so if necessary.Develop Terraform planAlright with the provider installed, it is time to use it to deploy a VM. Alter your main.tf file to be the following. I break it down inside the file with commentsmain.tf#Configure ProxMox provider using Telmate found here https://github.com/Telmate/terraform-provider-proxmoxterraform { required_providers { proxmox = { source = \"telmate/proxmox\" version = \"2.7.4\" } }}#Configure ProxMox API user/permissions and API urlprovider \"proxmox\" { pm_api_url = \"https://proxmox:8006/api2/json\" pm_api_token_id = \"terraform@pam!terraform_token\" pm_api_token_secret = \"&lt;your_api_token_secret&gt;\" pm_tls_insecure = true}#Configure Proxmox Resources Hereresource \"proxmox_vm_qemu\" \"Kubernetes-Node\" { count = 1 name = \"Kubernetes-Node-${count.index + 1}\" target_node = var.proxmox_host clone = var.template_name #Basic VM settings here agent = 1 # QEMU Guest Agent, 1 means installing the guest agent on this VM is set to True os_type = \"cloud-init\" cores = 2 sockets = 1 cpu = \"host\" memory = 2048 scsihw = \"virtio-scsi-pci\" bootdisk = \"scsi0\" disk { slot = 0 # set disk size here. leave it small for testing because expanding the disk takes time. size = \"10G\" type = \"scsi\" storage = \"local-lvm\" iothread = 1 } # if you want two NICs, just copy this whole network section and duplicate it network { model = \"virtio\" bridge = \"vmbr0\" } lifecycle { ignore_changes = [ network, ] } # sshkeys set using variables. the variable contains the text of the key. sshkeys = &lt;&lt;EOF ${var.ssh_key} EOF}Now for the vars.tf file. This is a bit easier to understand. Just declare a variable, give it a name, and a default value.vars.tfvariable \"ssh_key\" { default = \"ssh-rsa &lt;your_public_key_here&gt; brandon@ProxMox\"}variable \"proxmox_host\" { default = \"ProxMox\"}variable \"template_name\" { default = \"KubernetesNodeTemplate\"}Terraform planNow with the .tf files completed, we can run the plan (terraform plan). We defined a count=1 resource, so we would expect Terraform to create a single VM. Let’s have Terraform run through the plan and tell us what it intends to do. It tells us a lot.brandon@ProxMox:~/terraform# terraform planTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + createTerraform will perform the following actions: # proxmox_vm_qemu.Kubernetes-Node[0] will be created + resource \"proxmox_vm_qemu\" \"Kubernetes-Node\" { + additional_wait = 15 + agent = 1 + balloon = 0 + bios = \"seabios\" + boot = \"cdn\" + bootdisk = \"scsi0\" + clone = \"KubernetesNodeTemplate\" + clone_wait = 15 + cores = 2 + cpu = \"host\" + default_ipv4_address = (known after apply) + define_connection_info = true + force_create = false + full_clone = true + guest_agent_ready_timeout = 600 + hotplug = \"network,disk,usb\" + id = (known after apply) + kvm = true + memory = 2048 + name = \"Kubernetes-Node-1\" + nameserver = (known after apply) + numa = false + onboot = true + os_type = \"cloud-init\" + preprovision = true + reboot_required = (known after apply) + scsihw = \"virtio-scsi-pci\" + searchdomain = (known after apply) + sockets = 1 + ssh_host = (known after apply) + ssh_port = (known after apply) + sshkeys = &lt;&lt;-EOT ssh-rsa &lt;your_public_key_here&gt; brandon@ProxMox EOT + target_node = \"ProxMox\" + unused_disk = (known after apply) + vcpus = 0 + vlan = -1 + vmid = (known after apply) + disk { + backup = 0 + cache = \"none\" + file = (known after apply) + format = (known after apply) + iothread = 1 + mbps = 0 + mbps_rd = 0 + mbps_rd_max = 0 + mbps_wr = 0 + mbps_wr_max = 0 + media = (known after apply) + replicate = 0 + size = \"10G\" + slot = 0 + ssd = 0 + storage = \"local-lvm\" + storage_type = (known after apply) + type = \"scsi\" + volume = (known after apply) } + network { + bridge = \"vmbr0\" + firewall = false + link_down = false + macaddr = (known after apply) + model = \"virtio\" + queues = (known after apply) + rate = (known after apply) + tag = -1 } }Plan: 1 to add, 0 to change, 0 to destroy.───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraformapply\" now.You can see the output of the planning phase of Terraform. It is telling us it will create proxmox_vm_qemu.Kubernetes-Node[0] with a list of parameters. You can double-check the IP address here, as well as the rest of the basic settings. At the bottom is the summary – “Plan: 1 to add, 0 to change, 0 to destroy.” Also note that it tells us again what step to run next – “terraform apply”.Run Terraform plan and watch the VMs appearWith the summary stating what we want, we can now apply the plan (terraform apply). Note that it prompts you to type in ‘yes’ to apply the changes after it determines what the changes are. It typically takes 1m15s +/- 15s for my VMs to get created.If all goes well, you will be informed that 1 resource was added!Command and full output:brandon@ProxMox:~/terraform# terraform applyTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + createTerraform will perform the following actions: # proxmox_vm_qemu.Kubernetes-Node[0] will be created + resource \"proxmox_vm_qemu\" \"Kubernetes-Node\" { + additional_wait = 15 + agent = 1 + balloon = 0 + bios = \"seabios\" + boot = \"cdn\" + bootdisk = \"scsi0\" + clone = \"KubernetesNodeTemplate\" + clone_wait = 15 + cores = 2 + cpu = \"host\" + default_ipv4_address = (known after apply) + define_connection_info = true + force_create = false + full_clone = true + guest_agent_ready_timeout = 600 + hotplug = \"network,disk,usb\" + id = (known after apply) + kvm = true + memory = 2048 + name = \"Kubernetes-Node-1\" + nameserver = (known after apply) + numa = false + onboot = true + os_type = \"cloud-init\" + preprovision = true + reboot_required = (known after apply) + scsihw = \"virtio-scsi-pci\" + searchdomain = (known after apply) + sockets = 1 + ssh_host = (known after apply) + ssh_port = (known after apply) + sshkeys = &lt;&lt;-EOT ssh-rsa &lt;your_public_key_here&gt; brandon@ProxMox EOT + target_node = \"ProxMox\" + unused_disk = (known after apply) + vcpus = 0 + vlan = -1 + vmid = (known after apply) + disk { + backup = 0 + cache = \"none\" + file = (known after apply) + format = (known after apply) + iothread = 1 + mbps = 0 + mbps_rd = 0 + mbps_rd_max = 0 + mbps_wr = 0 + mbps_wr_max = 0 + media = (known after apply) + replicate = 0 + size = \"10G\" + slot = 0 + ssd = 0 + storage = \"local-lvm\" + storage_type = (known after apply) + type = \"scsi\" + volume = (known after apply) } + network { + bridge = \"vmbr0\" + firewall = false + link_down = false + macaddr = (known after apply) + model = \"virtio\" + queues = (known after apply) + rate = (known after apply) + tag = -1 } }Plan: 1 to add, 0 to change, 0 to destroy.Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yesproxmox_vm_qemu.Kubernetes-Node[0]: Creating...proxmox_vm_qemu.Kubernetes-Node[0]: Still creating... [10s elapsed]proxmox_vm_qemu.Kubernetes-Node[0]: Still creating... [20s elapsed]proxmox_vm_qemu.Kubernetes-Node[0]: Still creating... [30s elapsed]proxmox_vm_qemu.Kubernetes-Node[0]: Still creating... [40s elapsed]proxmox_vm_qemu.Kubernetes-Node[0]: Still creating... [50s elapsed]proxmox_vm_qemu.Kubernetes-Node[0]: Still creating... [1m0s elapsed]proxmox_vm_qemu.Kubernetes-Node[0]: Still creating... [1m10s elapsed]proxmox_vm_qemu.Kubernetes-Node[0]: Still creating... [1m20s elapsed]proxmox_vm_qemu.Kubernetes-Node[0]: Still creating... [1m30s elapsed]proxmox_vm_qemu.Kubernetes-Node[0]: Still creating... [1m40s elapsed]proxmox_vm_qemu.Kubernetes-Node[0]: Still creating... [1m50s elapsed]proxmox_vm_qemu.Kubernetes-Node[0]: Creation complete after 1m54s [id=ProxMox/qemu/100]Apply complete! Resources: 1 added, 0 changed, 0 destroyed.Now go check Proxmox and see if your VM was created:Success! You should now be able to SSH into the new VM with the key you already provided.Terraform DestroyTo remove a VM or anything created by Terraform with the .tf files we just used, we simply do the reverse of that by using the terraform destroy command.brandon@ProxMox:~/terraform# terraform destroyproxmox_vm_qemu.Kubernetes-Node[0]: Refreshing state... [id=ProxMox/qemu/100]Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroyTerraform will perform the following actions: # proxmox_vm_qemu.Kubernetes-Node[0] will be destroyed - resource \"proxmox_vm_qemu\" \"Kubernetes-Node\" { - additional_wait = 15 -&gt; null - agent = 1 -&gt; null - balloon = 0 -&gt; null - bios = \"seabios\" -&gt; null - boot = \"cdn\" -&gt; null - bootdisk = \"scsi0\" -&gt; null - ciuser = \"brandon\" -&gt; null - clone = \"KubernetesNodeTemplate\" -&gt; null - clone_wait = 15 -&gt; null - cores = 2 -&gt; null - cpu = \"host\" -&gt; null - default_ipv4_address = \"192.168.2.191\" -&gt; null - define_connection_info = true -&gt; null - disk_gb = 0 -&gt; null - force_create = false -&gt; null - full_clone = true -&gt; null - guest_agent_ready_timeout = 600 -&gt; null - hotplug = \"network,disk,usb\" -&gt; null - id = \"ProxMox/qemu/100\" -&gt; null - kvm = true -&gt; null - memory = 2048 -&gt; null - name = \"Kubernetes-Node-1\" -&gt; null - numa = false -&gt; null - onboot = true -&gt; null - os_type = \"cloud-init\" -&gt; null - preprovision = true -&gt; null - qemu_os = \"other\" -&gt; null - reboot_required = false -&gt; null - scsihw = \"virtio-scsi-pci\" -&gt; null - sockets = 1 -&gt; null - ssh_host = \"192.168.2.191\" -&gt; null - ssh_port = \"22\" -&gt; null - sshkeys = &lt;&lt;-EOT ssh-rsa &lt;your_public_key_here&gt; brandon@ProxMox EOT -&gt; null - target_node = \"ProxMox\" -&gt; null - unused_disk = [] -&gt; null - vcpus = 0 -&gt; null - vlan = -1 -&gt; null - disk { - backup = 0 -&gt; null - cache = \"none\" -&gt; null - file = \"vm-100-disk-0\" -&gt; null - format = \"raw\" -&gt; null - iothread = 1 -&gt; null - mbps = 0 -&gt; null - mbps_rd = 0 -&gt; null - mbps_rd_max = 0 -&gt; null - mbps_wr = 0 -&gt; null - mbps_wr_max = 0 -&gt; null - replicate = 0 -&gt; null - size = \"10G\" -&gt; null - slot = 0 -&gt; null - ssd = 0 -&gt; null - storage = \"local-lvm\" -&gt; null - storage_type = \"lvmthin\" -&gt; null - type = \"scsi\" -&gt; null - volume = \"local-lvm:vm-100-disk-0\" -&gt; null } - network { - bridge = \"vmbr0\" -&gt; null - firewall = false -&gt; null - link_down = false -&gt; null - macaddr = \"4E:54:B4:E7:91:17\" -&gt; null - model = \"virtio\" -&gt; null - queues = 0 -&gt; null - rate = 0 -&gt; null - tag = -1 -&gt; null } }Plan: 0 to add, 0 to change, 1 to destroy.Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only 'yes' will be accepted to confirm. Enter a value: yesproxmox_vm_qemu.Kubernetes-Node[0]: Destroying... [id=ProxMoxK8S/qemu/100]proxmox_vm_qemu.Kubernetes-Node[0]: Destruction complete after 4sDestroy complete! Resources: 1 destroyed." }, { "title": "Create a Debian Cloud-Init Template on Proxmox", "url": "/posts/ProxMoxCloudInitImage/", "categories": "Homelab, ProxMox", "tags": "hypervisor, proxmox, cloudinit, automation", "date": "2022-06-03 10:00:00 -0700", "snippet": "Cloud-Init is the de facto multi-distribution package that handles early initialization of a virtual machine instance. When the VM starts for the first time, the Cloud-Init software inside the VM will apply those settings.Proxmox templates together with Cloud-Init can be used to quickly deploy new VMs. A template quickly creates a new VM and Cloud-Init will initialize the new VM so that you only have to set the host name and the initial user account. No more installing the operating system from scratch for every new VM. In this guide, I’m describing how to do this with Debian to spin up headless Debian servers.Debian doesn’t provide a special image for this use case, but the Debian images designed for OpenStack/Cloud come with Cloud-Init support. Check out the Proxmox’s documentation for details on how Proxmox’s Cloud-Init support works.Table of Contents Download a base Debian cloud image How to get the latest link to use with wget How to download an image directly to ProxMox server Create a Proxmox VM using the image UsageDownload a base Debian cloud imageWe need to download the .qcow2 image file and that can be done from the following command:wget https://cloud.debian.org/images/cloud/bullseye/20220613-1045/debian-11-genericcloud-amd64-20220613-1045.qcow2You will see the following output after running the command.brandon@ProxMox:~# wget https://cloud.debian.org/images/cloud/bullseye/20220613-1045/debian-11-genericcloud-amd64-20220613-1045.qcow2--2022-06-17 19:51:13-- https://cloud.debian.org/images/cloud/bullseye/20220613-1045/debian-11-genericcloud-amd64-20220613-1045.qcow2Resolving cloud.debian.org (cloud.debian.org)... 194.71.11.173, 194.71.11.165, 194.71.11.163, ...Connecting to cloud.debian.org (cloud.debian.org)|194.71.11.173|:443... connected.HTTP request sent, awaiting response... 302 FoundLocation: https://laotzu.ftp.acc.umu.se/images/cloud/bullseye/20220613-1045/debian-11-genericcloud-amd64-20220613-1045.qcow2 [following]--2022-06-17 19:51:15-- https://laotzu.ftp.acc.umu.se/images/cloud/bullseye/20220613-1045/debian-11-genericcloud-amd64-20220613-1045.qcow2Resolving laotzu.ftp.acc.umu.se (laotzu.ftp.acc.umu.se)... 194.71.11.166, 2001:6b0:19::166Connecting to laotzu.ftp.acc.umu.se (laotzu.ftp.acc.umu.se)|194.71.11.166|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 253231104 (242M)Saving to: ‘debian-11-genericcloud-amd64-20220613-1045.qcow2’debian-11-genericcloud-a 100%[===============================&gt;] 241.50M 14.1MB/s in 19s 2022-06-17 19:51:37 (12.6 MB/s) - ‘debian-11-genericcloud-amd64-20220613-1045.qcow2’ saved [253231104/253231104]brandon@ProxMox:~# How to get the latest link to use with wgetTo be more specific on how we got the command to use from above, I will show you how to find the latest version of the Debian Cloud Image to use.Start by visiting the Debian Official Cloud Images page. The version I want to work with is Bullseye because this is the Debian 11 version and most current.Next click on the current version of the images which for me is 20220613-1045/Then choose the .qcow2 image and copy the link address by right clicking it. Paste that after the wget command in your ProxMox host.How to download an image directly to ProxMox serverAdditionally you can also use a feature in newer versions of ProxMox that will download from a link directly and then automatically add that image to the correct (/var/lib/vz/template/iso/) directory ProxMox checks in with during creation of a VM so will it appear on the list of image options. This is not strictly for the process we are working on right now, and can be used for simple regular Debian base images, or etc.You see the following output upon success:downloading https://cdimage.debian.org/debian-cd/current/amd64/iso-cd/debian-11.3.0-amd64-netinst.iso to /var/lib/vz/template/iso/debian-11.3.0-amd64-netinst.iso--2022-06-03 16:39:41-- https://cdimage.debian.org/debian-cd/current/amd64/iso-cd/debian-11.3.0-amd64-netinst.isoResolving cdimage.debian.org (cdimage.debian.org)... 194.71.11.173, 194.71.11.165, 194.71.11.163, ...Connecting to cdimage.debian.org (cdimage.debian.org)|194.71.11.173|:443... connected.HTTP request sent, awaiting response... 302 FoundLocation: https://laotzu.ftp.acc.umu.se/debian-cd/current/amd64/iso-cd/debian-11.3.0-amd64-netinst.iso [following]--2022-06-03 16:39:42-- https://laotzu.ftp.acc.umu.se/debian-cd/current/amd64/iso-cd/debian-11.3.0-amd64-netinst.isoResolving laotzu.ftp.acc.umu.se (laotzu.ftp.acc.umu.se)... 194.71.11.166, 2001:6b0:19::166Connecting to laotzu.ftp.acc.umu.se (laotzu.ftp.acc.umu.se)|194.71.11.166|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 396361728 (378M) [application/x-iso9660-image]Saving to: '/var/lib/vz/template/iso/debian-11.3.0-amd64-netinst.iso.tmp.480879' 0K ........ ........ ........ ........ 8% 8.19M 42s 32768K ........ ........ ........ ........ 16% 13.7M 31s 65536K ........ ........ ........ ........ 25% 13.7M 25s 98304K ........ ........ ........ ........ 33% 12.1M 22s131072K ........ ........ ........ ........ 42% 13.7M 18s163840K ........ ........ ........ ........ 50% 13.8M 15s196608K ........ ........ ........ ........ 59% 13.8M 13s229376K ........ ........ ........ ........ 67% 11.7M 10s262144K ........ ........ ........ ........ 76% 13.7M 7s294912K ........ ........ ........ ........ 84% 13.8M 5s327680K ........ ........ ........ ........ 93% 13.7M 2s360448K ........ ........ ........ .. 100% 11.2M=30s2022-06-03 16:40:15 (12.5 MB/s) - '/var/lib/vz/template/iso/debian-11.3.0-amd64-netinst.iso.tmp.480879' saved [396361728/396361728]download of 'https://cdimage.debian.org/debian-cd/current/amd64/iso-cd/debian-11.3.0-amd64-netinst.iso' to '/var/lib/vz/template/iso/debian-11.3.0-amd64-netinst.iso' finishedTASK OKCreate a Proxmox VM using the imageThe commands here should be relatively self explanatory but in general we are creating a VM (VMID=9500) with basic resources (2 cores, 2048MB), assigning networking to a virtio adapter on vmbr0, importing the image to storage (either local or local-lvm), setting disk 0 to use the image, setting boot drive to disk, setting the cloud init stuff to ide2 (which apparently appears as a CD-ROM to the VM, at least upon inital boot), and adding a virtual serial port.qm create 9500 --name Debian11CloudInit --net0 virtio,bridge=vmbr0qm importdisk 9500 debian-11-genericcloud-amd64-20220613-1045.qcow2 local-lvmqm set 9500 --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-9500-disk-0qm set 9500 --ide2 local-lvm:cloudinitqm set 9500 --boot c --bootdisk scsi0qm set 9500 --serial0 socket --vga serial0qm set 9500 --agent enabled=1 #optional but recommendedqm template 9500Here are the details to the command as they appear: Create a new VM with ID 900 using VirtIO networking drivers. Import the qcow Debian image as a disk to the new VM. The disk will be called local-lvm:vm-9500-disk-0. Attach the imported disk as a VirtIO SCSI device to the VM. Attach a drive for the Cloud-Init config to the VM. Set the VM to boot from the imported disk image. Add a serial console to the VM, which is needed by OpenStack/ProxMox. Enable the qemu-guest-agent for the VM – this is an optional setting, but I do recommend it because it will be useful if you are going to be using this for something like Terraform later on to automate the creation of VMs. Convert the VM into a template.UsageTo deploy a new server VM based on the template using the ID and name of your choice, execute the following command on the Proxmox host:qm clone 9500 9000 --name NEW-VMAfter the new VM is created, you can finish the setup in the Proxmox web interface with the following steps: In the Cloud-Init tab of the VM, configure the name of the default user and the public SSH key you want to use for authentication. In the Options tab, enable the QEMU Guest Agent if you did not from the commands above. In the Hardware tab, select the scsi0 hard disk and click Resize disk. The default size of the Debian image is 2 GiB. Specify the amount you want the disk to be increased by (e.g. 30 GiB for a total size of 32 GiB).Everything is ready to go! Start the VM, run a system upgrade, and install the QEMU guest agent:sudo apt updatesudo apt full-upgradesudo apt install qemu-guest-agent" }, { "title": "How to use Sony Remote Play on Linux", "url": "/posts/SonyRemotePlayOnLinux/", "categories": "Programming, Python", "tags": "python, sony, playstation, linux, chiaki, psn", "date": "2022-05-31 10:00:00 -0700", "snippet": "Chiaki is a Free and Open Source Software Client for PlayStation 4 and PlayStation 5 Remote Play for Linux, FreeBSD, OpenBSD, Android, macOS, Windows, Nintendo Switch and potentially even more platforms.InstallingYou can either download a pre-built release or build Chiaki from source. This download will be for the remote device you want to stream to.UsageIf your Console is on your local network, is turned on or in standby mode and does not have Discovery explicitly disabled, Chiaki should find it. Otherwise, you can add it manually. To do so, click the “+” icon in the top right, and enter your Console’s IP address.You will then need to register your Console with Chiaki. You will need two more pieces of information to do this.Obtaining your PSN AccountIDStarting with PS4 7.0, it is necessary to use a so-called “AccountID” as opposed to the “Online-ID” for registration (streaming itself did not change). This ID seems to be a unique identifier for a PSN Account and it can be obtained from the PSN after logging in using OAuth. A Python 3 script that does this is provided at psn-account-id.py or can be copied from below. Simply run it in a terminal and follow the instructions. Once you know your ID, write it down. You will likely never have to do this process again.#!/usr/bin/env python3# -*- coding: utf-8 -*-import sysif sys.version_info &lt; (3, 0, 0):\tprint(\"DO NOT use Python 2.\\nEVER.\\nhttps://pythonclock.org\")\texit(1)import platformoldexit = exitdef exit(code):\tif platform.system() == \"Windows\":\t\timport atexit\t\tinput(\"Press Enter to exit.\")\toldexit(code)if sys.stdout.encoding.lower() == \"ascii\":\timport codecs\tsys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)\tsys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)try:\timport requestsexcept ImportError as e:\tprint(e)\tif platform.system() == \"Windows\":\t\tfrom distutils.util import strtobool\t\ta = input(\"The requests module is not available. Should we try to install it automatically using pip? [y/n] \")\t\twhile True:\t\t\ttry:\t\t\t\ta = strtobool(a)\t\t\t\tbreak\t\t\texcept ValueError:\t\t\t\ta = input(\"Please answer with y or n: \")\t\tif a == 1:\t\t\timport subprocess\t\t\tsubprocess.call([sys.executable, \"-m\", \"pip\", \"install\", \"requests\"])\t\telse:\t\t\texit(1)\telse:\t\tprint(\"\\\"requests\\\" is not available. Install it with pip or your distribution's package manager.\")\t\texit(1)import requestsfrom urllib.parse import urlparse, parse_qs, quote, urljoinimport pprintimport base64# Remote Play Windows ClientCLIENT_ID = \"ba495a24-818c-472b-b12d-ff231c1b5745\"CLIENT_SECRET = \"mvaiZkRsAsI1IBkY\"LOGIN_URL = \"https://auth.api.sonyentertainmentnetwork.com/2.0/oauth/authorize?service_entity=urn:service-entity:psn&amp;response_type=code&amp;client_id={}&amp;redirect_uri=https://remoteplay.dl.playstation.net/remoteplay/redirect&amp;scope=psn:clientapp&amp;request_locale=en_US&amp;ui=pr&amp;service_logo=ps&amp;layout_type=popup&amp;smcid=remoteplay&amp;prompt=always&amp;PlatformPrivacyWs1=minimal&amp;\".format(CLIENT_ID)TOKEN_URL = \"https://auth.api.sonyentertainmentnetwork.com/2.0/oauth/token\"print()print(\"########################################################\")print(\" Script to determine PSN AccountID\")print(\" thanks to grill2010\")print(\" (This script will perform network operations.)\")print(\"########################################################\")print()print(\"➡️ Open the following URL in your Browser and log in:\")print()print(LOGIN_URL)print()print(\"➡️ After logging in, when the page shows \\\"redirect\\\", copy the URL from the address bar and paste it here:\")code_url_s = input(\"&gt; \")code_url = urlparse(code_url_s)query = parse_qs(code_url.query)if \"code\" not in query or len(query[\"code\"]) == 0 or len(query[\"code\"][0]) == 0:\tprint(\"☠️ URL did not contain code parameter\")\texit(1)code = query[\"code\"][0]print(\"🌏 Requesting OAuth Token\") api_auth = requests.auth.HTTPBasicAuth(CLIENT_ID, CLIENT_SECRET)body = \"grant_type=authorization_code&amp;code={}&amp;redirect_uri=https://remoteplay.dl.playstation.net/remoteplay/redirect&amp;\".format(code)token_request = requests.post(TOKEN_URL,\tauth = api_auth,\theaders = { \"Content-Type\": \"application/x-www-form-urlencoded\" },\tdata = body.encode(\"ascii\"))print(\"⚠️ WARNING: From this point on, output might contain sensitive information in some cases!\")if token_request.status_code != 200:\tprint(\"☠️ Request failed with code {}:\\n{}\".format(token_request.status_code, token_request.text))\texit(1)token_json = token_request.json()if \"access_token\" not in token_json:\tprint(\"☠️ \\\"access_token\\\" is missing in response JSON:\\n{}\".format(token_request.text))\texit(1)token = token_json[\"access_token\"]print(\"🌏 Requesting Account Info\")account_request = requests.get(TOKEN_URL + \"/\" + quote(token), auth = api_auth)if account_request.status_code != 200:\tprint(\"☠️ Request failed with code {}:\\n{}\".format(account_request.status_code, account_request.text))\texit(1)account_info = account_request.json()print(\"🥦 Received Account Info:\")pprint.pprint(account_info)if \"user_id\" not in account_info:\tprint(\"☠️ \\\"user_id\\\" is missing in response or not a string\")\texit(1)user_id = int(account_info[\"user_id\"])user_id_base64 = base64.b64encode(user_id.to_bytes(8, \"little\")).decode()print()print(\"🍙 This is your AccountID:\")print(user_id_base64)exit(0)1.Run the Python Script in a Terminal. Note I’m using Visual Studio Code to run this, so I am able to click the link directly from the terminal to visit the necessary page.2.Sign into your Sony Playstation Account.3.Paste the URL you had after the “Redirect” back into the temrinal with the Python Script. This will output information for the account you signed into, including the “AccountID” that we need for connecting Chiaki.Obtaining a Registration PINTo register a Console with a PIN, it must be put into registration mode. To do this on a PS4, simply go to: Settings -&gt; Remote Play -&gt; Add Device, or on a PS5: Settings -&gt; System -&gt; Remote Play -&gt; Link Device.You can now double-click your Console in Chiaki’s main window to start Remote Play." } ]
